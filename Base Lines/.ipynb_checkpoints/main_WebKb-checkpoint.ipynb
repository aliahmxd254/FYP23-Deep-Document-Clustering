{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03187fa7-9a21-4ab6-8c52-360ddadc03be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from nltk.corpus import wordnet as wn, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score as nmi_score\n",
    "from sklearn.metrics import adjusted_rand_score as ari_score\n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pickle\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn.cluster._kmeans\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn.feature_extraction.text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2ab4137-bd68-44fe-a88b-ed2438182213",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_content = []  # all the content in the document\n",
    "doc_name = []  # name of the document\n",
    "files_path = []  # path to the documents\n",
    "lexical_chain = []  # list of lexical chains from each document\n",
    "total_features = []  # total number of features. 1652\n",
    "final_training_Features = []\n",
    "corpus = []\n",
    "doc_list_sequence = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4acb809-b1f7-4032-94d1-ea607a3bbb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadDocuments(dir_name):\n",
    "    for Path in os.listdir(dir_name + '\\\\'):\n",
    "        file_p = os.getcwd() + f\"\\{dir_name}\\\\\" + Path\n",
    "        for files in os.listdir(file_p):\n",
    "            file = os.path.join(file_p, files)\n",
    "            files_path.append(file)\n",
    "            with open(file, 'r', encoding='utf-8') as file:\n",
    "                FileContents = file.read()\n",
    "                doc_content.append(FileContents.lower())\n",
    "                doc_name.append(Path)\n",
    "                doc_list_sequence.append(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "315666ad-0a28-4049-bb09-d62c0f264dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Purity_Score(label_seq, pred_labels):\n",
    "    # Calculate the confusion matrix to compare true labels and cluster assignments\n",
    "    confusion = confusion_matrix(label_seq, pred_labels)\n",
    "    # Calculate the purity\n",
    "    purity = np.sum(np.max(confusion, axis=0)) / np.sum(confusion)\n",
    "    return purity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ef42dd7-478c-4253-bb5e-e20e6cf510d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate(X, true_labels, predicted_labels):\n",
    "    purity = Purity_Score(true_labels, predicted_labels)\n",
    "    silhouette = silhouette_score(X, predicted_labels, metric='euclidean')\n",
    "    ari = ari_score(true_labels, predicted_labels)\n",
    "    nmi = nmi_score(true_labels, predicted_labels)\n",
    "    \n",
    "    print(f\"Purity: {purity}\")\n",
    "    print(f\"Silhouette Score: {silhouette}\")\n",
    "    print(f\"ARI Score: {ari}\")\n",
    "    print(f\"NMI Score: {nmi}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f339edc-8e86-43c2-8f17-4e49f7a80ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveFeatures(X, file_name):\n",
    "    pickle_path = open(file_name, 'wb')\n",
    "    pickle.dump(X, pickle_path)\n",
    "    pickle_path.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31e69287-affd-4db5-bbbc-be810a3507b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadFeatures(file_name):\n",
    "    pickle_read = open(file_name, 'rb')\n",
    "    x = pickle.load(pickle_read)\n",
    "    pickle_read.close()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3310a511-5e4b-4e69-b1b5-1b8fa0d97f6b",
   "metadata": {},
   "source": [
    "## Lexical Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d43efa23-6b91-448e-a511-76377d7eca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str, remove_stopwords: bool) -> str:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(\"[^A-Za-z]+\", \" \", text)\n",
    "    if remove_stopwords:\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        updated_tokens = []\n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i].lower() in stopwords.words(\"english\"):\n",
    "                continue\n",
    "            else:\n",
    "                updated_tokens.append(lemmatizer.lemmatize(tokens[i].lower()))\n",
    "\n",
    "    return updated_tokens\n",
    "\n",
    "\n",
    "def buildRelation(nouns):\n",
    "    relation_list = defaultdict(list)\n",
    "\n",
    "    for k in range(len(nouns)):\n",
    "        relation = []\n",
    "        for syn in wn.synsets(nouns[k], pos=wn.NOUN):\n",
    "            for l in syn.lemmas():\n",
    "                relation.append(l.name())\n",
    "                if l.antonyms():\n",
    "                    relation.append(l.antonyms()[0].name())\n",
    "            for l in syn.hyponyms():\n",
    "                if l.hyponyms():\n",
    "                    relation.append(l.hyponyms()[0].name().split(\".\")[0])\n",
    "            for l in syn.hypernyms():\n",
    "                if l.hypernyms():\n",
    "                    relation.append(l.hypernyms()[0].name().split(\".\")[0])\n",
    "        relation_list[nouns[k]].append(relation)\n",
    "    return relation_list\n",
    "\n",
    "\n",
    "def buildLexicalChain(nouns, relation_list):\n",
    "    lexical = []\n",
    "    threshold = 0.5\n",
    "    for noun in nouns:\n",
    "        flag = 0\n",
    "        for j in range(len(lexical)):\n",
    "            if flag == 0:\n",
    "                for key in list(lexical[j]):\n",
    "                    if key == noun and flag == 0:\n",
    "                        lexical[j][noun] += 1\n",
    "                        flag = 1\n",
    "                    elif key in relation_list[noun][0] and flag == 0:\n",
    "                        syns1 = wn.synsets(key, pos=wn.NOUN)\n",
    "                        syns2 = wn.synsets(noun, pos=wn.NOUN)\n",
    "                        if syns1[0].wup_similarity(syns2[0]) >= threshold:\n",
    "                            lexical[j][noun] = 1\n",
    "                            flag = 1\n",
    "                    elif noun in relation_list[key][0] and flag == 0:\n",
    "                        syns1 = wn.synsets(key, pos=wn.NOUN)\n",
    "                        syns2 = wn.synsets(noun, pos=wn.NOUN)\n",
    "                        if syns1[0].wup_similarity(syns2[0]) >= threshold:\n",
    "                            lexical[j][noun] = 1\n",
    "                            flag = 1\n",
    "        if flag == 0:\n",
    "            dic_nuevo = {}\n",
    "            dic_nuevo[noun] = 1\n",
    "            lexical.append(dic_nuevo)\n",
    "            flag = 1\n",
    "    return lexical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb28a4dd-55b4-4356-b70c-43613bea1f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminateWords(lexical):\n",
    "    final_chain = []\n",
    "    while lexical:\n",
    "        result = lexical.pop()\n",
    "        if len(result.keys()) == 1:\n",
    "            for value in result.values():\n",
    "                if value != 1:\n",
    "                    final_chain.append(result)\n",
    "        else:\n",
    "            final_chain.append(result)\n",
    "    return final_chain\n",
    "\n",
    "\n",
    "def PreprocessDocuments():\n",
    "    for i in files_path:\n",
    "        with open(i, \"r\", encoding='utf-8') as file:\n",
    "            dataset = preprocess_text(file.read(), remove_stopwords=True)\n",
    "        # use lexical chains as the feature selection method\n",
    "        nouns = []\n",
    "        l = nltk.pos_tag(dataset)\n",
    "        for word, n in l:\n",
    "            if n == \"NN\" or n == \"NNS\" or n == \"NNP\" or n == \"NNPS\":\n",
    "                nouns.append(word)\n",
    "\n",
    "        relation = buildRelation(nouns)\n",
    "        lexical = buildLexicalChain(nouns, relation)\n",
    "        chain = eliminateWords(lexical)\n",
    "        lexical_chain.append(chain)\n",
    "\n",
    "    global total_features\n",
    "    for features in lexical_chain:\n",
    "        for docfeature in features:\n",
    "            total_features.extend(docfeature.keys())\n",
    "\n",
    "    total_features = list(set(total_features))\n",
    "\n",
    "    for feature in lexical_chain:\n",
    "        temp = []\n",
    "        # print(feature)\n",
    "        for j in total_features:\n",
    "            check = False\n",
    "            for f in feature:\n",
    "                if j in f:\n",
    "                    temp.append(f[j])\n",
    "                    check = True\n",
    "                    break\n",
    "            if not check:\n",
    "                temp.append(0)\n",
    "\n",
    "        final_training_Features.append(temp)\n",
    "\n",
    "\n",
    "def build_lexical_chains(doc):\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    chains = {}\n",
    "\n",
    "    for token, pos in pos_tags:\n",
    "        synsets = wn.synsets(token, pos=wn.NOUN)\n",
    "        for synset in synsets:\n",
    "            if synset not in chains:\n",
    "                chains[synset] = [token]\n",
    "            else:\n",
    "                chains[synset].append(token)\n",
    "\n",
    "    return chains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5f2074d-2476-444d-b7e6-ad4445fac932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Documents...\n",
      "Building Relations...\n"
     ]
    }
   ],
   "source": [
    "WebKb_path = \"WebKB\"\n",
    "print(\"Reading Documents...\")\n",
    "ReadDocuments(WebKb_path)\n",
    "print(\"Building Relations...\")\n",
    "PreprocessDocuments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6657ddd-d779-4de8-8065-d2180dd11f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "normalize_features = normalizer.fit_transform(final_training_Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d7653ee-e232-45ef-8157-7c36334680d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SaveFeatures(final_training_Features, 'WebKB_Features_LexicalChains.pkl')\n",
    "# SaveFeatures(normalize_features, 'WebKB_Normalized_Features_LexicalChains.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fad431b1-df04-4371-92a8-d601ed38bd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_training_Features = ReadFeatures('WebKB_Features_LexicalChains.pkl')\n",
    "normalize_features = ReadFeatures('WebKB_Normalized_Features_LexicalChains.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05c68661-0d7a-4382-9481-027325728946",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_labels = {} # dictionary to store true assignments for each document | read sequence not followed\n",
    "dir_num = 0\n",
    "\n",
    "label_path = os.path.join(os.getcwd(),'WebKB')\n",
    "for labels_directory in os.listdir(label_path): # for each assignment folder\n",
    "    actual_cluster = dir_num \n",
    "    doc_labels = os.listdir(label_path + f\"\\\\{labels_directory}\") # for all document ids assigned to this cluster\n",
    "    for doc in doc_labels:\n",
    "        actual_labels[doc] = actual_cluster # save cluster label\n",
    "    dir_num+=1\n",
    "label_seq = [] # save labels in order of documents read\n",
    "for doc in doc_list_sequence:\n",
    "    label_seq.append(actual_labels[doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4fd32eda-9e38-4341-9b8e-a9c8ad18909f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8282"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1a2fc69-91f6-45d5-9c6f-91ebd9fcdb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Kmeans Clustering...\n",
      "Maximum purity of 0.5400869355228206 found on random state 343\n",
      "Purity: 0.5400869355228206\n",
      "Silhouette Score: 0.12729090342431598\n",
      "ARI Score: 0.1591516593740077\n",
      "NMI Score: 0.15750300304197687\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=30, random_state=42)\n",
    "pca_vecs = pca.fit_transform(normalize_features)\n",
    "\n",
    "print(\"Applying Kmeans Clustering...\")\n",
    "purity_collection = {}\n",
    "for i in range(700):\n",
    "    clusters = KMeans(n_init=\"auto\", n_clusters=5, random_state=i, init=\"k-means++\").fit(pca_vecs).labels_\n",
    "    purity_collection[i] = Purity_Score(label_seq, clusters)\n",
    "\n",
    "max_rand_state = max(purity_collection, key=purity_collection.get)\n",
    "print(f\"Maximum purity of {purity_collection[max_rand_state]} found on random state {max_rand_state}\")\n",
    "\n",
    "lexicalChainsLabels = KMeans(n_init=\"auto\", n_clusters=5, random_state=max_rand_state, init=\"k-means++\").fit(pca_vecs).labels_\n",
    "\n",
    "Evaluate(pca_vecs, label_seq, lexicalChainsLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf96313-f1ea-4977-ba8c-4da20209bb4b",
   "metadata": {},
   "source": [
    "## TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f8ea2d3-8735-4654-8424-b164bf7a4e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_wordnet(word):\n",
    "    synsets = wn.synsets(word)\n",
    "    return len(synsets) > 0\n",
    "\n",
    "def contains_number(word):\n",
    "    for char in word:\n",
    "        if char.isnumeric():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def min_length_word(word):\n",
    "    if  len(word) in [1,2]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def custom_preprocessor(text):\n",
    "    lematizer = WordNetLemmatizer()\n",
    "    used_terms = {} # keep track of which terms have already been considered\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = []\n",
    "    for word in tokens:\n",
    "        if (not contains_number(word)) and (not min_length_word(word)) and (word not in stopwords.words('english')) and (in_wordnet(word)):\n",
    "            lema_word = lematizer.lemmatize(word)\n",
    "            if lema_word in used_terms.keys():\n",
    "                continue\n",
    "            else:\n",
    "                used_terms[lema_word] = 0\n",
    "                filtered_tokens.append(lema_word)\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "def print_terms(terms):\n",
    "    for term in terms:\n",
    "        print(term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e96cb1-c4f8-488b-a1c8-49680cd0ca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KMeans_Labels(X, n, rstate_limit, true_labels):\n",
    "\n",
    "    # Specify the number of clusters (you can choose an appropriate value)\n",
    "    num_clusters = n\n",
    "    \n",
    "    # find centoids which give maximum purity\n",
    "    purity_collection = {}\n",
    "    for i in range(rstate_limit):\n",
    "        clusters = KMeans(n_init='auto', n_clusters=num_clusters, random_state=i, init='k-means++').fit(X).labels_\n",
    "        purity_collection[i] = Purity_Score(true_labels, clusters)\n",
    "    \n",
    "    max_rand_state = max(purity_collection, key=purity_collection.get)\n",
    "    print(f\"Maximum purity of {purity_collection[max_rand_state]} found on random state {max_rand_state}\")\n",
    "\n",
    "    # Create a KMeans model\n",
    "    kmeans = KMeans(n_init='auto', n_clusters=num_clusters, random_state=max_rand_state, init='k-means++')\n",
    "    # Fit the KMeans model to the TF-IDF data\n",
    "    kmeans.fit(X)\n",
    "    # Get the cluster assignments for each document\n",
    "    cluster_assignments = kmeans.labels_\n",
    "    \n",
    "    return cluster_assignments\n",
    "\n",
    "def print_results(true_labels, predicted_labels, X):\n",
    "    print(\"RESULTS:\")\n",
    "    print(f\"Purity: {Purity_Score(true_labels, predicted_labels)}\")\n",
    "    print(f\"Silhouette Score: {silhouette_score(X, predicted_labels)}\")\n",
    "\n",
    "\n",
    "def wrapperFunction():\n",
    "    # ReadDocuments('WebKB')\n",
    "    #vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', preprocessor=custom_preprocessor)\n",
    "    #X = vectorizer.fit_transform(doc_content)\n",
    "    #SaveFeatures(X, 'WebKB_TFIDF_Features.pkl')\n",
    "\n",
    "    X = ReadFeatures('WebKB_TFIDF_Features.pkl')\n",
    "    \n",
    "    true_labels = label_seq\n",
    "    predicted_labels = KMeans_Labels(X, 5, 700, true_labels)\n",
    "    Evaluate(X, true_labels, predicted_labels)\n",
    "    return predicted_labels, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df90b534-4f08-4a64-86b4-de07a009ea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfLabels, tfidfMatrix = wrapperFunction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b88aa3f-fd49-48bc-b830-a52cdeee64ed",
   "metadata": {},
   "source": [
    "## Consensus Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c0082f-69f7-4050-a5d0-d9ae0d5ba594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_consensus_matrix(labels1, labels2):\n",
    "    n = len(labels1)\n",
    "    consensus_matrix = np.zeros((n, n))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            #Calculate the Jaccard similarity between the two label sets\n",
    "            intersection = np.intersect1d(labels1[i], labels2[j])\n",
    "            union = np.union1d(labels1[i], labels2[j])\n",
    "            agreement = len(intersection) / len(union)\n",
    "        \n",
    "\n",
    "            consensus_matrix[i, j] = agreement\n",
    "            consensus_matrix[j, i] = agreement\n",
    "\n",
    "    return consensus_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1f8867-1357-438f-b23f-69c40534fb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_matrix = calculate_consensus_matrix(tfidfLabels, lexicalChainsLabels)\n",
    "\n",
    "n_clusters = 5  # You can adjust this as needed\n",
    "purity_collection = {}\n",
    "for i in range(1500):\n",
    "    clusters = SpectralClustering(n_clusters=n_clusters, affinity=\"precomputed\", random_state=i).fit(1 - consensus_matrix).labels_\n",
    "    purity_collection[i] = Purity_Score(label_seq, clusters)\n",
    "\n",
    "max_rand_state = max(purity_collection, key=purity_collection.get)\n",
    "print(f\"Maximum purity of {purity_collection[max_rand_state]} found on random state {max_rand_state}\")\n",
    "spectral_labels = SpectralClustering(n_clusters=n_clusters, affinity=\"precomputed\", random_state=max_rand_state).fit(1 - consensus_matrix).labels_\n",
    "\n",
    "Evaluate(1-consensus_matrix, label_seq, spectral_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29591cc-5112-41a0-8e64-760c1e86c700",
   "metadata": {},
   "source": [
    "## Topical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baf9c36-c84a-4f79-babf-dbb6dcca02c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 5  # Adjust as needed\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "\n",
    "lda.fit(tfidfMatrix)\n",
    "\n",
    "# Get the topic assignments for each document\n",
    "topic_labels = lda.transform(tfidfMatrix).argmax(axis=1)\n",
    "\n",
    "combined_labels = [lexicalChainsLabels, tfidfLabels, topic_labels]\n",
    "combined_labels = list(map(list, zip(*combined_labels)))\n",
    "\n",
    "normalize_combined_features = Normalizer().fit_transform(combined_labels)\n",
    "topic_purity_collection = {}\n",
    "for i in range(1500):\n",
    "    topic_clusters = (KMeans(n_init=\"auto\", n_clusters=5, random_state=i, init=\"k-means++\").fit(normalize_combined_features).labels_)\n",
    "    topic_purity_collection[i] = Purity_Score(label_seq, topic_clusters)\n",
    "\n",
    "topic_max_rand_state = max(topic_purity_collection, key=topic_purity_collection.get)\n",
    "print(f\"Maximum purity of {topic_purity_collection[topic_max_rand_state]} found on random state {topic_max_rand_state}\")\n",
    "max_labels = (KMeans(n_init=\"auto\", n_clusters=5, random_state=topic_max_rand_state, init=\"k-means++\").fit(normalize_combined_features).labels_)\n",
    "\n",
    "Evaluate(normalize_combined_features, label_seq, max_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
