{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03187fa7-9a21-4ab6-8c52-360ddadc03be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from nltk.corpus import wordnet as wn, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score as nmi_score\n",
    "from sklearn.metrics import adjusted_rand_score as ari_score\n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pickle\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn.cluster._kmeans\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn.feature_extraction.text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2ab4137-bd68-44fe-a88b-ed2438182213",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_content = []  # all the content in the document\n",
    "doc_name = []  # name of the document\n",
    "files_path = []  # path to the documents\n",
    "lexical_chain = []  # list of lexical chains from each document\n",
    "total_features = []  # total number of features. 1652\n",
    "final_training_Features = []\n",
    "corpus = []\n",
    "doc_list_sequence = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4acb809-b1f7-4032-94d1-ea607a3bbb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadDocuments(dir_name):\n",
    "    for Path in os.listdir(dir_name + '\\\\'):\n",
    "        file_p = os.getcwd() + f\"\\{dir_name}\\\\\" + Path\n",
    "        for files in os.listdir(file_p):\n",
    "            file = os.path.join(file_p, files)\n",
    "            files_path.append(file)\n",
    "            with open(file, 'r', encoding='utf-8') as file:\n",
    "                FileContents = file.read()\n",
    "                doc_content.append(FileContents.lower())\n",
    "                doc_name.append(Path)\n",
    "                doc_list_sequence.append(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "315666ad-0a28-4049-bb09-d62c0f264dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Purity_Score(label_seq, pred_labels):\n",
    "    # Calculate the confusion matrix to compare true labels and cluster assignments\n",
    "    confusion = confusion_matrix(label_seq, pred_labels)\n",
    "    # Calculate the purity\n",
    "    purity = np.sum(np.max(confusion, axis=0)) / np.sum(confusion)\n",
    "    return purity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ef42dd7-478c-4253-bb5e-e20e6cf510d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate(X, true_labels, predicted_labels):\n",
    "    purity = Purity_Score(true_labels, predicted_labels)\n",
    "    silhouette = silhouette_score(X, predicted_labels, metric='euclidean')\n",
    "    ari = ari_score(true_labels, predicted_labels)\n",
    "    nmi = nmi_score(true_labels, predicted_labels)\n",
    "    \n",
    "    print(f\"Purity: {purity}\")\n",
    "    print(f\"Silhouette Score: {silhouette}\")\n",
    "    print(f\"ARI Score: {ari}\")\n",
    "    print(f\"NMI Score: {nmi}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f339edc-8e86-43c2-8f17-4e49f7a80ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveFeatures(X, file_name):\n",
    "    pickle_path = open(file_name, 'wb')\n",
    "    pickle.dump(X, pickle_path)\n",
    "    pickle_path.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31e69287-affd-4db5-bbbc-be810a3507b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadFeatures(file_name):\n",
    "    pickle_read = open(file_name, 'rb')\n",
    "    x = pickle.load(pickle_read)\n",
    "    pickle_read.close()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3310a511-5e4b-4e69-b1b5-1b8fa0d97f6b",
   "metadata": {},
   "source": [
    "## Lexical Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d43efa23-6b91-448e-a511-76377d7eca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str, remove_stopwords: bool) -> str:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(\"[^A-Za-z]+\", \" \", text)\n",
    "    if remove_stopwords:\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        updated_tokens = []\n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i].lower() in stopwords.words(\"english\"):\n",
    "                continue\n",
    "            else:\n",
    "                updated_tokens.append(lemmatizer.lemmatize(tokens[i].lower()))\n",
    "\n",
    "    return updated_tokens\n",
    "\n",
    "\n",
    "def buildRelation(nouns):\n",
    "    relation_list = defaultdict(list)\n",
    "\n",
    "    for k in range(len(nouns)):\n",
    "        relation = []\n",
    "        for syn in wn.synsets(nouns[k], pos=wn.NOUN):\n",
    "            for l in syn.lemmas():\n",
    "                relation.append(l.name())\n",
    "                if l.antonyms():\n",
    "                    relation.append(l.antonyms()[0].name())\n",
    "            for l in syn.hyponyms():\n",
    "                if l.hyponyms():\n",
    "                    relation.append(l.hyponyms()[0].name().split(\".\")[0])\n",
    "            for l in syn.hypernyms():\n",
    "                if l.hypernyms():\n",
    "                    relation.append(l.hypernyms()[0].name().split(\".\")[0])\n",
    "        relation_list[nouns[k]].append(relation)\n",
    "    return relation_list\n",
    "\n",
    "\n",
    "def buildLexicalChain(nouns, relation_list):\n",
    "    lexical = []\n",
    "    threshold = 0.5\n",
    "    for noun in nouns:\n",
    "        flag = 0\n",
    "        for j in range(len(lexical)):\n",
    "            if flag == 0:\n",
    "                for key in list(lexical[j]):\n",
    "                    if key == noun and flag == 0:\n",
    "                        lexical[j][noun] += 1\n",
    "                        flag = 1\n",
    "                    elif key in relation_list[noun][0] and flag == 0:\n",
    "                        syns1 = wn.synsets(key, pos=wn.NOUN)\n",
    "                        syns2 = wn.synsets(noun, pos=wn.NOUN)\n",
    "                        if syns1[0].wup_similarity(syns2[0]) >= threshold:\n",
    "                            lexical[j][noun] = 1\n",
    "                            flag = 1\n",
    "                    elif noun in relation_list[key][0] and flag == 0:\n",
    "                        syns1 = wn.synsets(key, pos=wn.NOUN)\n",
    "                        syns2 = wn.synsets(noun, pos=wn.NOUN)\n",
    "                        if syns1[0].wup_similarity(syns2[0]) >= threshold:\n",
    "                            lexical[j][noun] = 1\n",
    "                            flag = 1\n",
    "        if flag == 0:\n",
    "            dic_nuevo = {}\n",
    "            dic_nuevo[noun] = 1\n",
    "            lexical.append(dic_nuevo)\n",
    "            flag = 1\n",
    "    return lexical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb28a4dd-55b4-4356-b70c-43613bea1f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminateWords(lexical):\n",
    "    final_chain = []\n",
    "    while lexical:\n",
    "        result = lexical.pop()\n",
    "        if len(result.keys()) == 1:\n",
    "            for value in result.values():\n",
    "                if value != 1:\n",
    "                    final_chain.append(result)\n",
    "        else:\n",
    "            final_chain.append(result)\n",
    "    return final_chain\n",
    "\n",
    "\n",
    "def PreprocessDocuments():\n",
    "    for i in files_path:\n",
    "        with open(i, \"r\", encoding='utf-8') as file:\n",
    "            dataset = preprocess_text(file.read(), remove_stopwords=True)\n",
    "        # use lexical chains as the feature selection method\n",
    "        nouns = []\n",
    "        l = nltk.pos_tag(dataset)\n",
    "        for word, n in l:\n",
    "            if n == \"NN\" or n == \"NNS\" or n == \"NNP\" or n == \"NNPS\":\n",
    "                nouns.append(word)\n",
    "\n",
    "        relation = buildRelation(nouns)\n",
    "        lexical = buildLexicalChain(nouns, relation)\n",
    "        chain = eliminateWords(lexical)\n",
    "        lexical_chain.append(chain)\n",
    "\n",
    "    global total_features\n",
    "    for features in lexical_chain:\n",
    "        for docfeature in features:\n",
    "            total_features.extend(docfeature.keys())\n",
    "\n",
    "    total_features = list(set(total_features))\n",
    "\n",
    "    for feature in lexical_chain:\n",
    "        temp = []\n",
    "        # print(feature)\n",
    "        for j in total_features:\n",
    "            check = False\n",
    "            for f in feature:\n",
    "                if j in f:\n",
    "                    temp.append(f[j])\n",
    "                    check = True\n",
    "                    break\n",
    "            if not check:\n",
    "                temp.append(0)\n",
    "\n",
    "        final_training_Features.append(temp)\n",
    "\n",
    "\n",
    "def build_lexical_chains(doc):\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    chains = {}\n",
    "\n",
    "    for token, pos in pos_tags:\n",
    "        synsets = wn.synsets(token, pos=wn.NOUN)\n",
    "        for synset in synsets:\n",
    "            if synset not in chains:\n",
    "                chains[synset] = [token]\n",
    "            else:\n",
    "                chains[synset].append(token)\n",
    "\n",
    "    return chains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5f2074d-2476-444d-b7e6-ad4445fac932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Documents...\n",
      "Building Relations...\n"
     ]
    }
   ],
   "source": [
    "WebKb_path = \"WebKB\"\n",
    "print(\"Reading Documents...\")\n",
    "ReadDocuments(WebKb_path)\n",
    "print(\"Building Relations...\")\n",
    "PreprocessDocuments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6657ddd-d779-4de8-8065-d2180dd11f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "normalize_features = normalizer.fit_transform(final_training_Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d7653ee-e232-45ef-8157-7c36334680d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SaveFeatures(final_training_Features, 'WebKB_Features_LexicalChains.pkl')\n",
    "# SaveFeatures(normalize_features, 'WebKB_Normalized_Features_LexicalChains.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fad431b1-df04-4371-92a8-d601ed38bd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_training_Features = ReadFeatures('WebKB_Features_LexicalChains.pkl')\n",
    "normalize_features = ReadFeatures('WebKB_Normalized_Features_LexicalChains.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05c68661-0d7a-4382-9481-027325728946",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_labels = {} # dictionary to store true assignments for each document | read sequence not followed\n",
    "dir_num = 0\n",
    "\n",
    "label_path = os.path.join(os.getcwd(),'WebKB')\n",
    "for labels_directory in os.listdir(label_path): # for each assignment folder\n",
    "    actual_cluster = dir_num \n",
    "    doc_labels = os.listdir(label_path + f\"\\\\{labels_directory}\") # for all document ids assigned to this cluster\n",
    "    for doc in doc_labels:\n",
    "        actual_labels[doc] = actual_cluster # save cluster label\n",
    "    dir_num+=1\n",
    "label_seq = [] # save labels in order of documents read\n",
    "for doc in doc_list_sequence:\n",
    "    label_seq.append(actual_labels[doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f1a2fc69-91f6-45d5-9c6f-91ebd9fcdb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Kmeans Clustering...\n",
      "Maximum purity of 0.5391209852692587 found on random state 221\n",
      "Purity: 0.5391209852692587\n",
      "Silhouette Score: 0.1526184179189444\n",
      "ARI Score: 0.15611769144041573\n",
      "NMI Score: 0.14122245615129927\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=30, random_state=42)\n",
    "pca_vecs = pca.fit_transform(normalize_features)\n",
    "\n",
    "print(\"Applying Kmeans Clustering...\")\n",
    "purity_collection = {}\n",
    "for i in range(700):\n",
    "    clusters = KMeans(n_init=\"auto\", n_clusters=7, random_state=i, init=\"k-means++\").fit(pca_vecs).labels_\n",
    "    purity_collection[i] = Purity_Score(label_seq, clusters)\n",
    "\n",
    "max_rand_state = max(purity_collection, key=purity_collection.get)\n",
    "print(f\"Maximum purity of {purity_collection[max_rand_state]} found on random state {max_rand_state}\")\n",
    "\n",
    "lexicalChainsLabels = KMeans(n_init=\"auto\", n_clusters=7, random_state=max_rand_state, init=\"k-means++\").fit(pca_vecs).labels_\n",
    "\n",
    "Evaluate(pca_vecs, label_seq, lexicalChainsLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf96313-f1ea-4977-ba8c-4da20209bb4b",
   "metadata": {},
   "source": [
    "## TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f8ea2d3-8735-4654-8424-b164bf7a4e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_wordnet(word):\n",
    "    synsets = wn.synsets(word)\n",
    "    return len(synsets) > 0\n",
    "\n",
    "def contains_number(word):\n",
    "    for char in word:\n",
    "        if char.isnumeric():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def min_length_word(word):\n",
    "    if  len(word) in [1,2]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def custom_preprocessor(text):\n",
    "    lematizer = WordNetLemmatizer()\n",
    "    used_terms = {} # keep track of which terms have already been considered\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = []\n",
    "    for word in tokens:\n",
    "        if (not contains_number(word)) and (not min_length_word(word)) and (word not in stopwords.words('english')) and (in_wordnet(word)):\n",
    "            lema_word = lematizer.lemmatize(word)\n",
    "            if lema_word in used_terms.keys():\n",
    "                continue\n",
    "            else:\n",
    "                used_terms[lema_word] = 0\n",
    "                filtered_tokens.append(lema_word)\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "def print_terms(terms):\n",
    "    for term in terms:\n",
    "        print(term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2e96cb1-c4f8-488b-a1c8-49680cd0ca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KMeans_Labels(X, n, rstate_limit, true_labels):\n",
    "\n",
    "    # Specify the number of clusters (you can choose an appropriate value)\n",
    "    num_clusters = n\n",
    "    \n",
    "    # find centoids which give maximum purity\n",
    "    purity_collection = {}\n",
    "    for i in range(rstate_limit):\n",
    "        clusters = KMeans(n_init='auto', n_clusters=num_clusters, random_state=i, init='k-means++').fit(X).labels_\n",
    "        purity_collection[i] = Purity_Score(true_labels, clusters)\n",
    "    \n",
    "    max_rand_state = max(purity_collection, key=purity_collection.get)\n",
    "    print(f\"Maximum purity of {purity_collection[max_rand_state]} found on random state {max_rand_state}\")\n",
    "\n",
    "    # Create a KMeans model\n",
    "    kmeans = KMeans(n_init='auto', n_clusters=num_clusters, random_state=max_rand_state, init='k-means++')\n",
    "    # Fit the KMeans model to the TF-IDF data\n",
    "    kmeans.fit(X)\n",
    "    # Get the cluster assignments for each document\n",
    "    cluster_assignments = kmeans.labels_\n",
    "    \n",
    "    return cluster_assignments\n",
    "\n",
    "def print_results(true_labels, predicted_labels, X):\n",
    "    print(\"RESULTS:\")\n",
    "    print(f\"Purity: {Purity_Score(true_labels, predicted_labels)}\")\n",
    "    print(f\"Silhouette Score: {silhouette_score(X, predicted_labels)}\")\n",
    "\n",
    "\n",
    "def wrapperFunction():\n",
    "    # ReadDocuments('WebKB')\n",
    "    #print(\"Building TF-IDF of Documents....\")\n",
    "    #vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', preprocessor=custom_preprocessor)\n",
    "    #X = vectorizer.fit_transform(doc_content)\n",
    "\n",
    "    #print(\"Saving Features...\")\n",
    "    #SaveFeatures(X, 'WebKB_TFIDF_Features.pkl')\n",
    "\n",
    "    X = ReadFeatures('WebKB_TFIDF_Features.pkl')\n",
    "    print(\"Applying Kmeans Clustering...\")\n",
    "    true_labels = label_seq\n",
    "    predicted_labels = KMeans_Labels(X, 7, 700, true_labels)\n",
    "    Evaluate(X, true_labels, predicted_labels)\n",
    "    return predicted_labels, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "df90b534-4f08-4a64-86b4-de07a009ea15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building TF-IDF of Documents....\n",
      "Saving Features...\n",
      "Applying Kmeans Clustering...\n",
      "Maximum purity of 0.6761651774933591 found on random state 71\n",
      "Purity: 0.6761651774933591\n",
      "Silhouette Score: 0.01206404904992725\n",
      "ARI Score: 0.26483536600457835\n",
      "NMI Score: 0.3335608713532205\n"
     ]
    }
   ],
   "source": [
    "tfidfLabels, tfidfMatrix = wrapperFunction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b88aa3f-fd49-48bc-b830-a52cdeee64ed",
   "metadata": {},
   "source": [
    "## Consensus Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "47c0082f-69f7-4050-a5d0-d9ae0d5ba594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_consensus_matrix(labels1, labels2):\n",
    "    n = len(labels1)\n",
    "    consensus_matrix = np.zeros((n, n))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            #Calculate the Jaccard similarity between the two label sets\n",
    "            intersection = np.intersect1d(labels1[i], labels2[j])\n",
    "            union = np.union1d(labels1[i], labels2[j])\n",
    "            agreement = len(intersection) / len(union)\n",
    "        \n",
    "\n",
    "            consensus_matrix[i, j] = agreement\n",
    "            consensus_matrix[j, i] = agreement\n",
    "\n",
    "    return consensus_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1a1f8867-1357-438f-b23f-69c40534fb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying Clustering on random state 0\n",
      "Trying Clustering on random state 1\n",
      "Trying Clustering on random state 2\n",
      "Trying Clustering on random state 3\n",
      "Trying Clustering on random state 4\n",
      "Trying Clustering on random state 5\n",
      "Trying Clustering on random state 6\n",
      "Trying Clustering on random state 7\n",
      "Trying Clustering on random state 8\n",
      "Trying Clustering on random state 9\n",
      "Trying Clustering on random state 10\n",
      "Trying Clustering on random state 11\n",
      "Trying Clustering on random state 12\n",
      "Trying Clustering on random state 13\n",
      "Trying Clustering on random state 14\n",
      "Trying Clustering on random state 15\n",
      "Trying Clustering on random state 16\n",
      "Trying Clustering on random state 17\n",
      "Trying Clustering on random state 18\n",
      "Trying Clustering on random state 19\n",
      "Trying Clustering on random state 20\n",
      "Trying Clustering on random state 21\n",
      "Trying Clustering on random state 22\n",
      "Trying Clustering on random state 23\n",
      "Trying Clustering on random state 24\n",
      "Trying Clustering on random state 25\n",
      "Trying Clustering on random state 26\n",
      "Trying Clustering on random state 27\n",
      "Trying Clustering on random state 28\n",
      "Trying Clustering on random state 29\n",
      "Trying Clustering on random state 30\n",
      "Trying Clustering on random state 31\n",
      "Trying Clustering on random state 32\n",
      "Trying Clustering on random state 33\n",
      "Trying Clustering on random state 34\n",
      "Trying Clustering on random state 35\n",
      "Trying Clustering on random state 36\n",
      "Trying Clustering on random state 37\n",
      "Trying Clustering on random state 38\n",
      "Trying Clustering on random state 39\n",
      "Trying Clustering on random state 40\n",
      "Trying Clustering on random state 41\n",
      "Trying Clustering on random state 42\n",
      "Trying Clustering on random state 43\n",
      "Trying Clustering on random state 44\n",
      "Trying Clustering on random state 45\n",
      "Trying Clustering on random state 46\n",
      "Trying Clustering on random state 47\n",
      "Trying Clustering on random state 48\n",
      "Trying Clustering on random state 49\n",
      "Trying Clustering on random state 50\n",
      "Trying Clustering on random state 51\n",
      "Trying Clustering on random state 52\n",
      "Trying Clustering on random state 53\n",
      "Trying Clustering on random state 54\n",
      "Trying Clustering on random state 55\n",
      "Trying Clustering on random state 56\n",
      "Trying Clustering on random state 57\n",
      "Trying Clustering on random state 58\n",
      "Trying Clustering on random state 59\n",
      "Trying Clustering on random state 60\n",
      "Trying Clustering on random state 61\n",
      "Trying Clustering on random state 62\n",
      "Trying Clustering on random state 63\n",
      "Trying Clustering on random state 64\n",
      "Trying Clustering on random state 65\n",
      "Trying Clustering on random state 66\n",
      "Trying Clustering on random state 67\n",
      "Trying Clustering on random state 68\n",
      "Trying Clustering on random state 69\n",
      "Trying Clustering on random state 70\n",
      "Trying Clustering on random state 71\n",
      "Trying Clustering on random state 72\n",
      "Trying Clustering on random state 73\n",
      "Trying Clustering on random state 74\n",
      "Trying Clustering on random state 75\n",
      "Trying Clustering on random state 76\n",
      "Trying Clustering on random state 77\n",
      "Trying Clustering on random state 78\n",
      "Trying Clustering on random state 79\n",
      "Trying Clustering on random state 80\n",
      "Trying Clustering on random state 81\n",
      "Trying Clustering on random state 82\n",
      "Trying Clustering on random state 83\n",
      "Trying Clustering on random state 84\n",
      "Trying Clustering on random state 85\n",
      "Trying Clustering on random state 86\n",
      "Trying Clustering on random state 87\n",
      "Trying Clustering on random state 88\n",
      "Trying Clustering on random state 89\n",
      "Trying Clustering on random state 90\n",
      "Trying Clustering on random state 91\n",
      "Trying Clustering on random state 92\n",
      "Trying Clustering on random state 93\n",
      "Trying Clustering on random state 94\n",
      "Trying Clustering on random state 95\n",
      "Trying Clustering on random state 96\n",
      "Trying Clustering on random state 97\n",
      "Trying Clustering on random state 98\n",
      "Trying Clustering on random state 99\n",
      "Trying Clustering on random state 100\n",
      "Trying Clustering on random state 101\n",
      "Trying Clustering on random state 102\n",
      "Trying Clustering on random state 103\n",
      "Trying Clustering on random state 104\n",
      "Trying Clustering on random state 105\n",
      "Trying Clustering on random state 106\n",
      "Trying Clustering on random state 107\n",
      "Trying Clustering on random state 108\n",
      "Trying Clustering on random state 109\n",
      "Trying Clustering on random state 110\n",
      "Trying Clustering on random state 111\n",
      "Trying Clustering on random state 112\n",
      "Trying Clustering on random state 113\n",
      "Trying Clustering on random state 114\n",
      "Trying Clustering on random state 115\n",
      "Trying Clustering on random state 116\n",
      "Trying Clustering on random state 117\n",
      "Trying Clustering on random state 118\n",
      "Trying Clustering on random state 119\n",
      "Trying Clustering on random state 120\n",
      "Trying Clustering on random state 121\n",
      "Trying Clustering on random state 122\n",
      "Trying Clustering on random state 123\n",
      "Trying Clustering on random state 124\n",
      "Trying Clustering on random state 125\n",
      "Trying Clustering on random state 126\n",
      "Trying Clustering on random state 127\n",
      "Trying Clustering on random state 128\n",
      "Trying Clustering on random state 129\n",
      "Trying Clustering on random state 130\n",
      "Trying Clustering on random state 131\n",
      "Trying Clustering on random state 132\n",
      "Trying Clustering on random state 133\n",
      "Trying Clustering on random state 134\n",
      "Trying Clustering on random state 135\n",
      "Trying Clustering on random state 136\n",
      "Trying Clustering on random state 137\n",
      "Trying Clustering on random state 138\n",
      "Trying Clustering on random state 139\n",
      "Trying Clustering on random state 140\n",
      "Trying Clustering on random state 141\n",
      "Trying Clustering on random state 142\n",
      "Trying Clustering on random state 143\n",
      "Trying Clustering on random state 144\n",
      "Trying Clustering on random state 145\n",
      "Trying Clustering on random state 146\n",
      "Trying Clustering on random state 147\n",
      "Trying Clustering on random state 148\n",
      "Trying Clustering on random state 149\n",
      "Trying Clustering on random state 150\n",
      "Trying Clustering on random state 151\n",
      "Trying Clustering on random state 152\n",
      "Trying Clustering on random state 153\n",
      "Trying Clustering on random state 154\n",
      "Trying Clustering on random state 155\n",
      "Trying Clustering on random state 156\n",
      "Trying Clustering on random state 157\n",
      "Trying Clustering on random state 158\n",
      "Trying Clustering on random state 159\n",
      "Trying Clustering on random state 160\n",
      "Trying Clustering on random state 161\n",
      "Trying Clustering on random state 162\n",
      "Trying Clustering on random state 163\n",
      "Trying Clustering on random state 164\n",
      "Trying Clustering on random state 165\n",
      "Trying Clustering on random state 166\n",
      "Trying Clustering on random state 167\n",
      "Trying Clustering on random state 168\n",
      "Trying Clustering on random state 169\n",
      "Trying Clustering on random state 170\n",
      "Trying Clustering on random state 171\n",
      "Trying Clustering on random state 172\n",
      "Trying Clustering on random state 173\n",
      "Trying Clustering on random state 174\n",
      "Trying Clustering on random state 175\n",
      "Trying Clustering on random state 176\n",
      "Trying Clustering on random state 177\n",
      "Trying Clustering on random state 178\n",
      "Trying Clustering on random state 179\n",
      "Trying Clustering on random state 180\n",
      "Trying Clustering on random state 181\n",
      "Trying Clustering on random state 182\n",
      "Trying Clustering on random state 183\n",
      "Trying Clustering on random state 184\n",
      "Trying Clustering on random state 185\n",
      "Trying Clustering on random state 186\n",
      "Trying Clustering on random state 187\n",
      "Trying Clustering on random state 188\n",
      "Trying Clustering on random state 189\n",
      "Trying Clustering on random state 190\n",
      "Trying Clustering on random state 191\n",
      "Trying Clustering on random state 192\n",
      "Trying Clustering on random state 193\n",
      "Trying Clustering on random state 194\n",
      "Trying Clustering on random state 195\n",
      "Trying Clustering on random state 196\n",
      "Trying Clustering on random state 197\n",
      "Trying Clustering on random state 198\n",
      "Trying Clustering on random state 199\n",
      "Trying Clustering on random state 200\n",
      "Trying Clustering on random state 201\n",
      "Trying Clustering on random state 202\n",
      "Trying Clustering on random state 203\n",
      "Trying Clustering on random state 204\n",
      "Trying Clustering on random state 205\n",
      "Trying Clustering on random state 206\n",
      "Trying Clustering on random state 207\n",
      "Trying Clustering on random state 208\n",
      "Trying Clustering on random state 209\n",
      "Trying Clustering on random state 210\n",
      "Trying Clustering on random state 211\n",
      "Trying Clustering on random state 212\n",
      "Trying Clustering on random state 213\n",
      "Trying Clustering on random state 214\n",
      "Trying Clustering on random state 215\n",
      "Trying Clustering on random state 216\n",
      "Trying Clustering on random state 217\n",
      "Trying Clustering on random state 218\n",
      "Trying Clustering on random state 219\n",
      "Trying Clustering on random state 220\n",
      "Trying Clustering on random state 221\n",
      "Trying Clustering on random state 222\n",
      "Trying Clustering on random state 223\n",
      "Trying Clustering on random state 224\n",
      "Trying Clustering on random state 225\n",
      "Trying Clustering on random state 226\n",
      "Trying Clustering on random state 227\n",
      "Trying Clustering on random state 228\n",
      "Trying Clustering on random state 229\n",
      "Trying Clustering on random state 230\n",
      "Trying Clustering on random state 231\n",
      "Trying Clustering on random state 232\n",
      "Trying Clustering on random state 233\n",
      "Trying Clustering on random state 234\n",
      "Trying Clustering on random state 235\n",
      "Trying Clustering on random state 236\n",
      "Trying Clustering on random state 237\n",
      "Trying Clustering on random state 238\n",
      "Trying Clustering on random state 239\n",
      "Trying Clustering on random state 240\n",
      "Trying Clustering on random state 241\n",
      "Trying Clustering on random state 242\n",
      "Trying Clustering on random state 243\n",
      "Trying Clustering on random state 244\n",
      "Trying Clustering on random state 245\n",
      "Trying Clustering on random state 246\n",
      "Trying Clustering on random state 247\n",
      "Trying Clustering on random state 248\n",
      "Trying Clustering on random state 249\n",
      "Trying Clustering on random state 250\n",
      "Trying Clustering on random state 251\n",
      "Trying Clustering on random state 252\n",
      "Trying Clustering on random state 253\n",
      "Trying Clustering on random state 254\n",
      "Trying Clustering on random state 255\n",
      "Trying Clustering on random state 256\n",
      "Trying Clustering on random state 257\n",
      "Trying Clustering on random state 258\n",
      "Trying Clustering on random state 259\n",
      "Trying Clustering on random state 260\n",
      "Trying Clustering on random state 261\n",
      "Trying Clustering on random state 262\n",
      "Trying Clustering on random state 263\n",
      "Trying Clustering on random state 264\n",
      "Trying Clustering on random state 265\n",
      "Trying Clustering on random state 266\n",
      "Trying Clustering on random state 267\n",
      "Trying Clustering on random state 268\n",
      "Trying Clustering on random state 269\n",
      "Trying Clustering on random state 270\n",
      "Trying Clustering on random state 271\n",
      "Trying Clustering on random state 272\n",
      "Trying Clustering on random state 273\n",
      "Trying Clustering on random state 274\n",
      "Trying Clustering on random state 275\n",
      "Trying Clustering on random state 276\n",
      "Trying Clustering on random state 277\n",
      "Trying Clustering on random state 278\n",
      "Trying Clustering on random state 279\n",
      "Trying Clustering on random state 280\n",
      "Trying Clustering on random state 281\n",
      "Trying Clustering on random state 282\n",
      "Trying Clustering on random state 283\n",
      "Trying Clustering on random state 284\n",
      "Trying Clustering on random state 285\n",
      "Trying Clustering on random state 286\n",
      "Trying Clustering on random state 287\n",
      "Trying Clustering on random state 288\n",
      "Trying Clustering on random state 289\n",
      "Trying Clustering on random state 290\n",
      "Trying Clustering on random state 291\n",
      "Trying Clustering on random state 292\n",
      "Trying Clustering on random state 293\n",
      "Trying Clustering on random state 294\n",
      "Trying Clustering on random state 295\n",
      "Trying Clustering on random state 296\n",
      "Trying Clustering on random state 297\n",
      "Trying Clustering on random state 298\n",
      "Trying Clustering on random state 299\n",
      "Trying Clustering on random state 300\n",
      "Trying Clustering on random state 301\n",
      "Trying Clustering on random state 302\n",
      "Trying Clustering on random state 303\n",
      "Trying Clustering on random state 304\n",
      "Trying Clustering on random state 305\n",
      "Trying Clustering on random state 306\n",
      "Trying Clustering on random state 307\n",
      "Trying Clustering on random state 308\n",
      "Trying Clustering on random state 309\n",
      "Trying Clustering on random state 310\n",
      "Trying Clustering on random state 311\n",
      "Trying Clustering on random state 312\n",
      "Trying Clustering on random state 313\n",
      "Trying Clustering on random state 314\n",
      "Trying Clustering on random state 315\n",
      "Trying Clustering on random state 316\n",
      "Trying Clustering on random state 317\n",
      "Trying Clustering on random state 318\n",
      "Trying Clustering on random state 319\n",
      "Trying Clustering on random state 320\n",
      "Trying Clustering on random state 321\n",
      "Trying Clustering on random state 322\n",
      "Trying Clustering on random state 323\n",
      "Trying Clustering on random state 324\n",
      "Trying Clustering on random state 325\n",
      "Trying Clustering on random state 326\n",
      "Trying Clustering on random state 327\n",
      "Trying Clustering on random state 328\n",
      "Trying Clustering on random state 329\n",
      "Trying Clustering on random state 330\n",
      "Trying Clustering on random state 331\n",
      "Trying Clustering on random state 332\n",
      "Trying Clustering on random state 333\n",
      "Trying Clustering on random state 334\n",
      "Trying Clustering on random state 335\n",
      "Trying Clustering on random state 336\n",
      "Trying Clustering on random state 337\n",
      "Trying Clustering on random state 338\n",
      "Trying Clustering on random state 339\n",
      "Trying Clustering on random state 340\n",
      "Trying Clustering on random state 341\n",
      "Trying Clustering on random state 342\n",
      "Trying Clustering on random state 343\n",
      "Trying Clustering on random state 344\n",
      "Trying Clustering on random state 345\n",
      "Trying Clustering on random state 346\n",
      "Trying Clustering on random state 347\n",
      "Trying Clustering on random state 348\n",
      "Trying Clustering on random state 349\n",
      "Trying Clustering on random state 350\n",
      "Trying Clustering on random state 351\n",
      "Trying Clustering on random state 352\n",
      "Trying Clustering on random state 353\n",
      "Trying Clustering on random state 354\n",
      "Trying Clustering on random state 355\n",
      "Trying Clustering on random state 356\n",
      "Trying Clustering on random state 357\n",
      "Trying Clustering on random state 358\n",
      "Trying Clustering on random state 359\n",
      "Trying Clustering on random state 360\n",
      "Trying Clustering on random state 361\n",
      "Trying Clustering on random state 362\n",
      "Trying Clustering on random state 363\n",
      "Trying Clustering on random state 364\n",
      "Trying Clustering on random state 365\n",
      "Trying Clustering on random state 366\n",
      "Trying Clustering on random state 367\n",
      "Trying Clustering on random state 368\n",
      "Trying Clustering on random state 369\n",
      "Trying Clustering on random state 370\n",
      "Trying Clustering on random state 371\n",
      "Trying Clustering on random state 372\n",
      "Trying Clustering on random state 373\n",
      "Trying Clustering on random state 374\n",
      "Trying Clustering on random state 375\n",
      "Trying Clustering on random state 376\n",
      "Trying Clustering on random state 377\n",
      "Trying Clustering on random state 378\n",
      "Trying Clustering on random state 379\n",
      "Trying Clustering on random state 380\n",
      "Trying Clustering on random state 381\n",
      "Trying Clustering on random state 382\n",
      "Trying Clustering on random state 383\n",
      "Trying Clustering on random state 384\n",
      "Trying Clustering on random state 385\n",
      "Trying Clustering on random state 386\n",
      "Trying Clustering on random state 387\n",
      "Trying Clustering on random state 388\n",
      "Trying Clustering on random state 389\n",
      "Trying Clustering on random state 390\n",
      "Trying Clustering on random state 391\n",
      "Trying Clustering on random state 392\n",
      "Trying Clustering on random state 393\n",
      "Trying Clustering on random state 394\n",
      "Trying Clustering on random state 395\n",
      "Trying Clustering on random state 396\n",
      "Trying Clustering on random state 397\n",
      "Trying Clustering on random state 398\n",
      "Trying Clustering on random state 399\n",
      "Trying Clustering on random state 400\n",
      "Trying Clustering on random state 401\n",
      "Trying Clustering on random state 402\n",
      "Trying Clustering on random state 403\n",
      "Trying Clustering on random state 404\n",
      "Trying Clustering on random state 405\n",
      "Trying Clustering on random state 406\n",
      "Trying Clustering on random state 407\n",
      "Trying Clustering on random state 408\n",
      "Trying Clustering on random state 409\n",
      "Trying Clustering on random state 410\n",
      "Trying Clustering on random state 411\n",
      "Trying Clustering on random state 412\n",
      "Trying Clustering on random state 413\n",
      "Trying Clustering on random state 414\n",
      "Trying Clustering on random state 415\n",
      "Trying Clustering on random state 416\n",
      "Trying Clustering on random state 417\n",
      "Trying Clustering on random state 418\n",
      "Trying Clustering on random state 419\n",
      "Trying Clustering on random state 420\n",
      "Trying Clustering on random state 421\n",
      "Trying Clustering on random state 422\n",
      "Trying Clustering on random state 423\n",
      "Trying Clustering on random state 424\n",
      "Trying Clustering on random state 425\n",
      "Trying Clustering on random state 426\n",
      "Trying Clustering on random state 427\n",
      "Trying Clustering on random state 428\n",
      "Trying Clustering on random state 429\n",
      "Trying Clustering on random state 430\n",
      "Trying Clustering on random state 431\n",
      "Trying Clustering on random state 432\n",
      "Trying Clustering on random state 433\n",
      "Trying Clustering on random state 434\n",
      "Trying Clustering on random state 435\n",
      "Trying Clustering on random state 436\n",
      "Trying Clustering on random state 437\n",
      "Trying Clustering on random state 438\n",
      "Trying Clustering on random state 439\n",
      "Trying Clustering on random state 440\n",
      "Trying Clustering on random state 441\n",
      "Trying Clustering on random state 442\n",
      "Trying Clustering on random state 443\n",
      "Trying Clustering on random state 444\n",
      "Trying Clustering on random state 445\n",
      "Trying Clustering on random state 446\n",
      "Trying Clustering on random state 447\n",
      "Trying Clustering on random state 448\n",
      "Trying Clustering on random state 449\n",
      "Trying Clustering on random state 450\n",
      "Trying Clustering on random state 451\n",
      "Trying Clustering on random state 452\n",
      "Trying Clustering on random state 453\n",
      "Trying Clustering on random state 454\n",
      "Trying Clustering on random state 455\n",
      "Trying Clustering on random state 456\n",
      "Trying Clustering on random state 457\n",
      "Trying Clustering on random state 458\n",
      "Trying Clustering on random state 459\n",
      "Trying Clustering on random state 460\n",
      "Trying Clustering on random state 461\n",
      "Trying Clustering on random state 462\n",
      "Trying Clustering on random state 463\n",
      "Trying Clustering on random state 464\n",
      "Trying Clustering on random state 465\n",
      "Trying Clustering on random state 466\n",
      "Trying Clustering on random state 467\n",
      "Trying Clustering on random state 468\n",
      "Trying Clustering on random state 469\n",
      "Trying Clustering on random state 470\n",
      "Trying Clustering on random state 471\n",
      "Trying Clustering on random state 472\n",
      "Trying Clustering on random state 473\n",
      "Trying Clustering on random state 474\n",
      "Trying Clustering on random state 475\n",
      "Trying Clustering on random state 476\n",
      "Trying Clustering on random state 477\n",
      "Trying Clustering on random state 478\n",
      "Trying Clustering on random state 479\n",
      "Trying Clustering on random state 480\n",
      "Trying Clustering on random state 481\n",
      "Trying Clustering on random state 482\n",
      "Trying Clustering on random state 483\n",
      "Trying Clustering on random state 484\n",
      "Trying Clustering on random state 485\n",
      "Trying Clustering on random state 486\n",
      "Trying Clustering on random state 487\n",
      "Trying Clustering on random state 488\n",
      "Trying Clustering on random state 489\n",
      "Trying Clustering on random state 490\n",
      "Trying Clustering on random state 491\n",
      "Trying Clustering on random state 492\n",
      "Trying Clustering on random state 493\n",
      "Trying Clustering on random state 494\n",
      "Trying Clustering on random state 495\n",
      "Trying Clustering on random state 496\n",
      "Trying Clustering on random state 497\n",
      "Trying Clustering on random state 498\n",
      "Trying Clustering on random state 499\n",
      "Maximum purity of 0.730379135474523 found on random state 335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Lib\\site-packages\\sklearn\\utils\\extmath.py:193: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity: 0.730379135474523\n",
      "Silhouette Score: 0.195682459982077\n",
      "ARI Score: 0.2808571947823359\n",
      "NMI Score: 0.4154660139433615\n"
     ]
    }
   ],
   "source": [
    "# print(\"Building Consensus Matrix...\")\n",
    "# consensus_matrix = calculate_consensus_matrix(tfidfLabels, lexicalChainsLabels)\n",
    "# SaveFeatures(consensus_matrix, 'WebKB_Consensus_Matrix.pkl')\n",
    "\n",
    "consensus_matrix = ReadFeatures('WebKB_Consensus_Matrix.pkl')\n",
    "\n",
    "n_clusters = 7 # You can adjust this as needed\n",
    "purity_collection = {}\n",
    "for i in range(500):\n",
    "    print(f\"Trying Clustering on random state {i}\")\n",
    "    clusters = SpectralClustering(n_clusters=n_clusters, affinity=\"precomputed\", random_state=i).fit(1 - consensus_matrix).labels_\n",
    "    purity_collection[i] = Purity_Score(label_seq, clusters)\n",
    "\n",
    "max_rand_state = max(purity_collection, key=purity_collection.get)\n",
    "print(f\"Maximum purity of {purity_collection[max_rand_state]} found on random state {max_rand_state}\")\n",
    "spectral_labels = SpectralClustering(n_clusters=n_clusters, affinity=\"precomputed\", random_state=max_rand_state).fit(1 - consensus_matrix).labels_\n",
    "\n",
    "Evaluate(1-consensus_matrix, label_seq, spectral_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c0889340-7067-4591-950a-eb40606d2cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_matrix = calculate_consensus_matrix(tfidfLabels, lexicalChainsLabels)\n",
    "SaveFeatures(consensus_matrix, 'WebKB_Consensus_Matrix.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29591cc-5112-41a0-8e64-760c1e86c700",
   "metadata": {},
   "source": [
    "## Topical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4baf9c36-c84a-4f79-babf-dbb6dcca02c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum purity of 0.5638734605167833 found on random state 121\n",
      "Purity: 0.5638734605167833\n",
      "Silhouette Score: 0.5726049355353215\n",
      "ARI Score: 0.1388594059671092\n",
      "NMI Score: 0.2120688822825985\n"
     ]
    }
   ],
   "source": [
    "num_topics = 7  # Adjust as needed\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "\n",
    "lda.fit(tfidfMatrix)\n",
    "\n",
    "# Get the topic assignments for each document\n",
    "topic_labels = lda.transform(tfidfMatrix).argmax(axis=1)\n",
    "\n",
    "combined_labels = [lexicalChainsLabels, tfidfLabels, topic_labels]\n",
    "combined_labels = list(map(list, zip(*combined_labels)))\n",
    "\n",
    "normalize_combined_features = Normalizer().fit_transform(combined_labels)\n",
    "topic_purity_collection = {}\n",
    "for i in range(700):\n",
    "    topic_clusters = (KMeans(n_init=\"auto\", n_clusters=5, random_state=i, init=\"k-means++\").fit(normalize_combined_features).labels_)\n",
    "    topic_purity_collection[i] = Purity_Score(label_seq, topic_clusters)\n",
    "\n",
    "topic_max_rand_state = max(topic_purity_collection, key=topic_purity_collection.get)\n",
    "print(f\"Maximum purity of {topic_purity_collection[topic_max_rand_state]} found on random state {topic_max_rand_state}\")\n",
    "max_labels = (KMeans(n_init=\"auto\", n_clusters=5, random_state=topic_max_rand_state, init=\"k-means++\").fit(normalize_combined_features).labels_)\n",
    "\n",
    "Evaluate(normalize_combined_features, label_seq, max_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
