{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_content = []  # all the content in the document\n",
    "doc_name = []  # name of the document\n",
    "files_path = []  # path to the documents\n",
    "lexical_chain = []  # list of lexical chains from each document\n",
    "total_features = []  # total number of features. 1652\n",
    "final_training_Features = [] # to store features for training\n",
    "corpus = [] # to store all text\n",
    "doc_list_sequence = [] # store sequence of document read\n",
    "actual_labels = {} # to store actual cluster of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadDocuments(dir_name):\n",
    "    current_file_id = 0\n",
    "    current_dir_id = 0\n",
    "    for Path in os.listdir(dir_name):\n",
    "        sub_dir_path = os.path.join(dir_name, Path)\n",
    "        for sub_dir_files in os.listdir(sub_dir_path):\n",
    "            file_p = os.path.join(sub_dir_path, sub_dir_files)\n",
    "            with open(file_p, \"r\") as file:\n",
    "                FileContents = file.read()\n",
    "                doc_content.append(FileContents.lower())\n",
    "                doc_name.append(current_file_id)\n",
    "                actual_labels[current_file_id] = current_dir_id \n",
    "                current_file_id+=1\n",
    "                files_path.append(file_p)\n",
    "        current_dir_id+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_wordnet(word):\n",
    "    synsets = wn.synsets(word)\n",
    "    return len(synsets) > 0\n",
    "\n",
    "def contains_number(word):\n",
    "    for char in word:\n",
    "        if char.isnumeric():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def min_length_word(word):\n",
    "    if  len(word) in [1,2]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def custom_preprocessor(text):\n",
    "    lematizer = WordNetLemmatizer()\n",
    "    used_terms = {} # keep track of which terms have already been considered\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = []\n",
    "    for word in tokens:\n",
    "        if (not contains_number(word)) and (not min_length_word(word)) and (word not in stopwords.words('english')) and (in_wordnet(word)):\n",
    "            lema_word = lematizer.lemmatize(word)\n",
    "            if lema_word in used_terms.keys():\n",
    "                continue\n",
    "            else:\n",
    "                used_terms[lema_word] = 0\n",
    "                filtered_tokens.append(lema_word)\n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\umair\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ReadDocuments(os.getcwd() + \"\\BBC\")\n",
    "vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', preprocessor=custom_preprocessor)\n",
    "print(\"Building Features...\")\n",
    "X = vectorizer.fit_transform(doc_content)\n",
    "\n",
    "# # save tdidf-features\n",
    "# print(\"Saving tf-idf features\")\n",
    "# output_file_path = \"tfidf_output.txt\"\n",
    "# with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "#     np.savetxt(output_file, X.toarray(), fmt='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = \"bbc.txt\"\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    np.savetxt(output_file, X.toarray(), fmt='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "import os\n",
    "\n",
    "# Read the text file\n",
    "file_path = \"input_file.txt\"\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    documents = file.readlines()\n",
    "\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "\n",
    "# Write TF-IDF results to a new file\n",
    "output_file_path = \"bbc.txt\"\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    np.savetxt(output_file, tfidf_matrix.toarray(), fmt='%.4f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to c:\\Users\\umair\\Desktop\\FYP23-Deep-Document-Clustering\\Base Linesoutput.txt\n"
     ]
    }
   ],
   "source": [
    "file_name = os.getcwd() + \"output.txt\"\n",
    " \n",
    "# Open the file in write mode\n",
    "with open(file_name, \"w\") as file:\n",
    "    # Iterate over the dictionary items and write them to the file\n",
    "    for key, value in actual_labels.items():\n",
    "        file.write(f\"{value}\\n\")\n",
    "\n",
    "print(\"Data written to\", file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2225x16821 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 266246 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.getcwd() + '/cite.txt'\n",
    "def get_text_file_shape(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        num_lines = len(lines)\n",
    "        max_line_length = max(len(line) for line in lines)\n",
    "    return num_lines, max_line_length\n",
    "\n",
    "# num_lines, max_line_length = get_text_file_shape(file_path)\n",
    "# print(\"Number of lines:\", num_lines)\n",
    "# print(\"Maximum line length:\", max_line_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 9635\n",
      "Maximum line length: 25\n"
     ]
    }
   ],
   "source": [
    "file_path = os.getcwd() + '/bbc.terms'  # Replace \"your_file_path_here.term\" with the actual file path\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "except FileNotFoundError:\n",
    "    print(f\"File '{file_path}' not found.\")\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "\n",
    "num_lines, max_line_length = get_text_file_shape(file_path)\n",
    "print(\"Number of lines:\", num_lines)\n",
    "print(\"Maximum line length:\", max_line_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix Shape: (9635, 2225)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Load the Matrix Market file\n",
    "matrix = io.mmread(\"bbc.mtx\")  # Replace \"your_file.mtx\" with the actual file path\n",
    "\n",
    "# Initialize TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "# Fit and transform the data\n",
    "tfidf_matrix = tfidf_transformer.fit_transform(matrix)\n",
    "\n",
    "# Print the shape of the TF-IDF matrix\n",
    "print(\"TF-IDF Matrix Shape:\", tfidf_matrix.shape)\n",
    "\n",
    "output_file_path = \"bbc.txt\"\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    np.savetxt(output_file, tfidf_matrix.toarray(), fmt='%.4f')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
