{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali Ahmed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import warnings\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from nltk.corpus import wordnet as wn, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# from tf_idf import wrapperFunction\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import adjusted_rand_score  # Import the Rand Index metric\n",
    "from collections import Counter\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn.cluster._kmeans\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn.feature_extraction.text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_content = []  # all the content in the document\n",
    "doc_name = []  # name of the document\n",
    "files_path = []  # path to the documents\n",
    "lexical_chain = []  # list of lexical chains from each document\n",
    "total_features = []  # total number of features. 1652\n",
    "final_training_Features = []\n",
    "corpus = []\n",
    "doc_list_sequence = []\n",
    "\n",
    "data_dict = {}\n",
    "cluster_dict = {}\n",
    "mapped_data_dict = {}\n",
    "word_to_int = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadDocuments(dir_name):\n",
    "    for Path in os.listdir(dir_name):\n",
    "        file_p = os.path.join(dir_name, Path)\n",
    "        with open(file_p, \"r\") as file:\n",
    "            FileContents = file.read()\n",
    "            doc_content.append(FileContents.lower())\n",
    "            doc_name.append(Path)\n",
    "            files_path.append(file_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str, remove_stopwords: bool) -> str:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(\"[^A-Za-z]+\", \" \", text)\n",
    "    if remove_stopwords:\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        updated_tokens = []\n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i].lower() in stopwords.words(\"english\"):\n",
    "                continue\n",
    "            else:\n",
    "                updated_tokens.append(lemmatizer.lemmatize(tokens[i].lower()))\n",
    "\n",
    "    return updated_tokens\n",
    "\n",
    "def buildRelation(nouns):\n",
    "    relation_list = defaultdict(list)\n",
    "\n",
    "    for k in range(len(nouns)):\n",
    "        relation = []\n",
    "        for syn in wn.synsets(nouns[k], pos=wn.NOUN):\n",
    "            for l in syn.lemmas():\n",
    "                relation.append(l.name())\n",
    "                if l.antonyms():\n",
    "                    relation.append(l.antonyms()[0].name())\n",
    "            for l in syn.hyponyms():\n",
    "                if l.hyponyms():\n",
    "                    relation.append(l.hyponyms()[0].name().split(\".\")[0])\n",
    "            for l in syn.hypernyms():\n",
    "                if l.hypernyms():\n",
    "                    relation.append(l.hypernyms()[0].name().split(\".\")[0])\n",
    "        relation_list[nouns[k]].append(relation)\n",
    "    return relation_list\n",
    "\n",
    "def buildLexicalChain(nouns, relation_list):\n",
    "    lexical = []\n",
    "    threshold = 0.5\n",
    "    for noun in nouns:\n",
    "        flag = 0\n",
    "        for j in range(len(lexical)):\n",
    "            if flag == 0:\n",
    "                for key in list(lexical[j]):\n",
    "                    if key == noun and flag == 0:\n",
    "                        lexical[j][noun] += 1\n",
    "                        flag = 1\n",
    "                    elif key in relation_list[noun][0] and flag == 0:\n",
    "                        syns1 = wn.synsets(key, pos=wn.NOUN)\n",
    "                        syns2 = wn.synsets(noun, pos=wn.NOUN)\n",
    "                        if syns1[0].wup_similarity(syns2[0]) >= threshold:\n",
    "                            lexical[j][noun] = 1\n",
    "                            flag = 1\n",
    "                    elif noun in relation_list[key][0] and flag == 0:\n",
    "                        syns1 = wn.synsets(key, pos=wn.NOUN)\n",
    "                        syns2 = wn.synsets(noun, pos=wn.NOUN)\n",
    "                        if syns1[0].wup_similarity(syns2[0]) >= threshold:\n",
    "                            lexical[j][noun] = 1\n",
    "                            flag = 1\n",
    "        if flag == 0:\n",
    "            dic_nuevo = {}\n",
    "            dic_nuevo[noun] = 1\n",
    "            lexical.append(dic_nuevo)\n",
    "            flag = 1\n",
    "    return lexical\n",
    "\n",
    "def eliminateWords(lexical):\n",
    "    final_chain = []\n",
    "    while lexical:\n",
    "        result = lexical.pop()\n",
    "        if len(result.keys()) == 1:\n",
    "            for value in result.values():\n",
    "                if value != 1:\n",
    "                    final_chain.append(result)\n",
    "        else:\n",
    "            final_chain.append(result)\n",
    "    return final_chain\n",
    "\n",
    "def PreprocessDocuments():\n",
    "    for i in files_path:\n",
    "        f = open(i, \"r\")\n",
    "        dataset = preprocess_text(f.read(), remove_stopwords=True)\n",
    "        # use lexical chains as the feature selection method\n",
    "        nouns = []\n",
    "        l = nltk.pos_tag(dataset)\n",
    "        for word, n in l:\n",
    "            if n == \"NN\" or n == \"NNS\" or n == \"NNP\" or n == \"NNPS\":\n",
    "                nouns.append(word)\n",
    "\n",
    "        relation = buildRelation(nouns)\n",
    "        lexical = buildLexicalChain(nouns, relation)\n",
    "        chain = eliminateWords(lexical)\n",
    "        lexical_chain.append(chain)\n",
    "\n",
    "    global total_features\n",
    "    for features in lexical_chain:\n",
    "        for docfeature in features:\n",
    "            total_features.extend(docfeature.keys())\n",
    "\n",
    "    total_features = list(set(total_features))\n",
    "\n",
    "    for feature in lexical_chain:\n",
    "        temp = []\n",
    "        # print(feature)\n",
    "        for j in total_features:\n",
    "            check = False\n",
    "            for f in feature:\n",
    "                if j in f:\n",
    "                    temp.append(f[j])\n",
    "                    check = True\n",
    "                    break\n",
    "            if not check:\n",
    "                temp.append(0)\n",
    "\n",
    "        final_training_Features.append(temp)\n",
    "\n",
    "def build_lexical_chains(doc):\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    chains = {}\n",
    "\n",
    "    for token, pos in pos_tags:\n",
    "        synsets = wn.synsets(token, pos=wn.NOUN)\n",
    "        for synset in synsets:\n",
    "            if synset not in chains:\n",
    "                chains[synset] = [token]\n",
    "            else:\n",
    "                chains[synset].append(token)\n",
    "\n",
    "    return chains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Purity_Score(true_labels, predicted_labels):\n",
    "    total = len(true_labels)\n",
    "    num_clusters = len(set(predicted_labels))\n",
    "    \n",
    "    purity_sum = 0\n",
    "\n",
    "    for cluster in set(predicted_labels):\n",
    "        cluster_indices = [i for i, label in enumerate(predicted_labels) if label == cluster]\n",
    "        \n",
    "        if not cluster_indices:\n",
    "            continue  # Skip the cluster if there are no elements in it\n",
    "        \n",
    "        cluster_true_labels = [true_labels[i] for i in cluster_indices]\n",
    "        cluster_true_labels = [item for sublist in cluster_true_labels for item in sublist]  # Flatten the list of true labels\n",
    "        \n",
    "        class_counts = Counter(cluster_true_labels)\n",
    "        \n",
    "        # Check if class_counts is not empty before finding the maximum value\n",
    "        if class_counts:\n",
    "            max_class_count = max(class_counts.values())\n",
    "            purity_sum += max_class_count\n",
    "\n",
    "    return purity_sum / total\n",
    "\n",
    "\n",
    "# def Purity_Score(label_seq, pred_labels):\n",
    "#     # Calculate the confusion matrix to compare true labels and cluster assignments\n",
    "#     confusion = confusion_matrix(label_seq, pred_labels)\n",
    "#     # Calculate the purity\n",
    "#     purity = np.sum(np.max(confusion, axis=0)) / np.sum(confusion)\n",
    "#     return purity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetLabels():\n",
    "    with open('D:\\FAST\\FYP\\FYP23-Deep-Document-Clustering\\Base Lines\\Reuters\\cats.txt', 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) >= 2:\n",
    "            key = parts[0]\n",
    "            values = parts[1:]\n",
    "            if len(values) == 1:\n",
    "                data_dict[key] = values[0]\n",
    "            else:\n",
    "                data_dict[key] = values\n",
    "                for value in values:\n",
    "                    cluster_dict[value] = None\n",
    "\n",
    "    word_list = list(cluster_dict.keys())\n",
    "\n",
    "    for i, word in enumerate(word_list):\n",
    "        word_to_int[word] = i\n",
    "\n",
    "    for key, value in data_dict.items():\n",
    "        if isinstance(value, list):\n",
    "            mapped_values = [word_to_int[word] for word in value]\n",
    "            mapped_data_dict[key] = mapped_values\n",
    "        else:\n",
    "            value_list = [value] if not isinstance(value, list) else value\n",
    "            mapped_values = [word_to_int[word] if word in word_to_int else word for word in value_list]\n",
    "            mapped_data_dict[key] = mapped_values\n",
    "\n",
    "    label_seq = list(mapped_data_dict.values())\n",
    "    # label_seq = [item for sublist in label_seq for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "# print(\"word list\",len(word_list),word_list)\n",
    "# print(\"word to int\", word_to_int)\n",
    "# print(\"mapped values\", mapped_values)\n",
    "# print(\"mapped_data_dict\", mapped_data_dict)\n",
    "    return label_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_features():\n",
    "    file_path = r\"D:\\FAST\\FYP\\FYP23-Deep-Document-Clustering\\Base Lines\\Vectors.pkl\"\n",
    "    file = open(file_path, \"wb\")\n",
    "    pickle.dump(final_training_Features, file)\n",
    "    file.close()\n",
    "\n",
    "def read_features():\n",
    "    file_path = r\"D:\\FAST\\FYP\\FYP23-Deep-Document-Clustering\\Base Lines\\Vectors.pkl\"\n",
    "    a_file = open(file_path,\"rb\")\n",
    "    X = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "    return X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")      # GPU\n",
    "doc_path = \"D:\\FAST\\FYP\\FYP23-Deep-Document-Clustering\\Base Lines\\Reuters\\Training\"\n",
    "# doc_path = os.getcwd() + \"\\Reuters\\Training\"\n",
    "ReadDocuments(doc_path)\n",
    "PreprocessDocuments()\n",
    "# store_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_training_Features = read_features()\n",
    "# final_training_Features = torch.tensor(final_training_Features, dtype=torch.float32).to(device)      # GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "final_training_Features = normalizer.fit_transform(final_training_Features)\n",
    "# final_training_Features = final_training_Features / torch.norm(final_training_Features, dim=1)[:, None]      # GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum purity of 0.36985539488320357 found on random state 143\n",
      "K-means lables using lexical chains: [17 39 63 ... 44  8  0]\n",
      "      \n",
      "Purity 0.36985539488320357\n",
      "      \n",
      "Silhoutte Score: 0.05858385816446256\n"
     ]
    }
   ],
   "source": [
    "SumSqDis = []\n",
    "pca = PCA(n_components=30, random_state=42)\n",
    "pca_vecs = pca.fit_transform(final_training_Features)\n",
    "# pca_vecs = torch.tensor(pca_vecs, dtype=torch.float32).to(device)\n",
    "\n",
    "label_seq = GetLabels()\n",
    "\n",
    "purity_collection = {}\n",
    "for i in range(500):\n",
    "    clusters = KMeans(n_init=\"auto\", n_clusters=len(list(word_to_int.values())), random_state=i, init=\"k-means++\").fit(pca_vecs).labels_\n",
    "    purity_collection[i] = Purity_Score(label_seq, clusters)\n",
    "\n",
    "max_rand_state = max(purity_collection, key=purity_collection.get)\n",
    "print(\n",
    "    f\"Maximum purity of {purity_collection[max_rand_state]} found on random state {max_rand_state}\"\n",
    ")\n",
    "\n",
    "lexicalChainsLabels = KMeans(n_init=\"auto\", n_clusters=len(list(word_to_int.values())), random_state=max_rand_state, init=\"k-means++\").fit(pca_vecs).labels_\n",
    "\n",
    "print(f\"\"\"K-means lables using lexical chains: {lexicalChainsLabels}\n",
    "      \\nPurity {Purity_Score(label_seq, lexicalChainsLabels)}\n",
    "      \\nSilhoutte Score: {metrics.silhouette_score(final_training_Features, lexicalChainsLabels, metric='euclidean')}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_wordnet(word):\n",
    "    synsets = wn.synsets(word)\n",
    "    return len(synsets) > 0\n",
    "\n",
    "def contains_number(word):\n",
    "    for char in word:\n",
    "        if char.isnumeric():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def min_length_word(word):\n",
    "    if  len(word) in [1,2]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def custom_preprocessor(text):\n",
    "    lematizer = WordNetLemmatizer()\n",
    "    used_terms = {} # keep track of which terms have already been considered\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = []\n",
    "    for word in tokens:\n",
    "        if (not contains_number(word)) and (not min_length_word(word)) and (word not in stopwords.words('english')) and (in_wordnet(word)):\n",
    "            lema_word = lematizer.lemmatize(word)\n",
    "            if lema_word in used_terms.keys():\n",
    "                continue\n",
    "            else:\n",
    "                used_terms[lema_word] = 0\n",
    "                filtered_tokens.append(lema_word)\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "def print_terms(terms):\n",
    "    for term in terms:\n",
    "        print(term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KMeans_Labels(X, n, rstate_limit, true_labels):\n",
    "    # find centoids which give maximum purity\n",
    "    purity_collection = {}\n",
    "    for i in range(rstate_limit):\n",
    "        clusters = KMeans(n_init='auto', n_clusters=n, random_state=i, init='k-means++').fit(X).labels_\n",
    "        purity_collection[i] = Purity_Score(true_labels, clusters)\n",
    "    \n",
    "    max_rand_state = max(purity_collection, key=purity_collection.get)\n",
    "    print(f\"Maximum purity of {purity_collection[max_rand_state]} found on random state {max_rand_state}\")\n",
    "\n",
    "    # Create a KMeans model\n",
    "    cluster_assignments = KMeans(n_init='auto', n_clusters=n, random_state=max_rand_state, init='k-means++').fit(X).labels_\n",
    "\n",
    "    return cluster_assignments\n",
    "\n",
    "def print_results(true_labels, predicted_labels, X):\n",
    "    print(\"RESULTS:\")\n",
    "    print(f\"Purity: {Purity_Score(true_labels, predicted_labels)}\")\n",
    "    print(f\"Silhouette Score: {silhouette_score(X, predicted_labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', preprocessor=custom_preprocessor)\n",
    "X = vectorizer.fit_transform(doc_content)\n",
    "true_labels = GetLabels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum purity of 0.36772339636633294 found on random state 67\n",
      "RESULTS:\n",
      "Purity: 0.36772339636633294\n",
      "Silhouette Score: -0.006047238747527097\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = KMeans_Labels(X, 5, 100, true_labels)\n",
    "print_results(true_labels, predicted_labels, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfLabels = predicted_labels\n",
    "tfidfMatrix = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consensus Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_consensus_matrix(labels1, labels2):\n",
    "    n = len(labels1)\n",
    "    consensus_matrix = np.zeros((n, n))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            #Calculate the Jaccard similarity between the two label sets\n",
    "            intersection = np.intersect1d(labels1[i], labels2[j])\n",
    "            union = np.union1d(labels1[i], labels2[j])\n",
    "            agreement = len(intersection) / len(union)\n",
    "        \n",
    "\n",
    "            consensus_matrix[i, j] = agreement\n",
    "            consensus_matrix[j, i] = agreement\n",
    "\n",
    "    return consensus_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum purity of 0.36763070077864296 found on random state 2\n",
      "Purity Score:  0.36763070077864296\n",
      "Sillhouette Coefficient:  -0.11535524251258987\n"
     ]
    }
   ],
   "source": [
    "consensus_matrix = calculate_consensus_matrix(tfidfLabels, lexicalChainsLabels)\n",
    "\n",
    "n_clusters = 5  # You can adjust this as needed\n",
    "purity_collection = {}\n",
    "for i in range(500):\n",
    "    clusters = SpectralClustering(n_clusters=n_clusters, affinity=\"precomputed\", random_state=i).fit(1 - consensus_matrix).labels_\n",
    "    purity_collection[i] = Purity_Score(label_seq, clusters)\n",
    "\n",
    "max_rand_state = max(purity_collection, key=purity_collection.get)\n",
    "print(f\"Maximum purity of {purity_collection[max_rand_state]} found on random state {max_rand_state}\")\n",
    "spectral_labels = SpectralClustering(n_clusters=n_clusters, affinity=\"precomputed\", random_state=max_rand_state).fit(1 - consensus_matrix).labels_\n",
    "print(\"Purity Score: \", Purity_Score(label_seq, spectral_labels))\n",
    "print(\"Sillhouette Coefficient: \",metrics.silhouette_score(pca_vecs, spectral_labels, metric=\"euclidean\"),)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum purity of 0.3674453096032629 found on random state 0\n",
      "Purity:  0.3674453096032629\n",
      "Sillhouette Coefficient:  0.7289370934632332\n"
     ]
    }
   ],
   "source": [
    "num_topics = 5  # Adjust as needed\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "topic_proportions = lda.fit_transform(tfidfMatrix)\n",
    "\n",
    "combined_features = np.hstack((spectral_labels.reshape(-1, 1), topic_proportions))\n",
    "\n",
    "normalize_combined_features = Normalizer().fit_transform(combined_features)\n",
    "\n",
    "topic_purity_collection = {}\n",
    "for i in range(500):\n",
    "    topic_clusters = (\n",
    "        KMeans(n_init=\"auto\", n_clusters=5, random_state=i, init=\"k-means++\")\n",
    "        .fit(normalize_combined_features)\n",
    "        .labels_\n",
    "    )\n",
    "    topic_purity_collection[i] = Purity_Score(label_seq, topic_clusters)\n",
    "\n",
    "topic_max_rand_state = max(topic_purity_collection, key=topic_purity_collection.get)\n",
    "print(f\"Maximum purity of {topic_purity_collection[topic_max_rand_state]} found on random state {topic_max_rand_state}\")\n",
    "max_labels = (\n",
    "    KMeans(\n",
    "        n_init=\"auto\", n_clusters=5, random_state=topic_max_rand_state, init=\"k-means++\"\n",
    "    )\n",
    "    .fit(normalize_combined_features)\n",
    "    .labels_\n",
    ")\n",
    "print(\"Purity: \", Purity_Score(label_seq, max_labels))\n",
    "print(\"Sillhouette Coefficient: \",metrics.silhouette_score(normalize_combined_features, max_labels, metric=\"euclidean\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
