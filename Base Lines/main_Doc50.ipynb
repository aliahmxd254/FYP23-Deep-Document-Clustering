{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from nltk.corpus import wordnet as wn, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn.cluster._kmeans\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn.feature_extraction.text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_content = []  # all the content in the document\n",
    "doc_name = []  # name of the document\n",
    "files_path = []  # path to the documents\n",
    "lexical_chain = []  # list of lexical chains from each document\n",
    "total_features = []  # total number of features. 1652\n",
    "final_training_Features = []\n",
    "corpus = []\n",
    "doc_list_sequence = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadDocuments(dir_name):\n",
    "    for Path in os.listdir(dir_name):\n",
    "        file_p = os.path.join(dir_name, Path)\n",
    "        with open(file_p, \"r\") as file:\n",
    "            FileContents = file.read()\n",
    "            doc_content.append(FileContents.lower())\n",
    "            doc_name.append(Path)\n",
    "            files_path.append(file_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Purity_Score(label_seq, pred_labels):\n",
    "    # Calculate the confusion matrix to compare true labels and cluster assignments\n",
    "    confusion = confusion_matrix(label_seq, pred_labels)\n",
    "    # Calculate the purity\n",
    "    purity = np.sum(np.max(confusion, axis=0)) / np.sum(confusion)\n",
    "    return purity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str, remove_stopwords: bool) -> str:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(\"[^A-Za-z]+\", \" \", text)\n",
    "    if remove_stopwords:\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        updated_tokens = []\n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i].lower() in stopwords.words(\"english\"):\n",
    "                continue\n",
    "            else:\n",
    "                updated_tokens.append(lemmatizer.lemmatize(tokens[i].lower()))\n",
    "\n",
    "    return updated_tokens\n",
    "\n",
    "def buildRelation(nouns):\n",
    "    relation_list = defaultdict(list)\n",
    "\n",
    "    for k in range(len(nouns)):\n",
    "        relation = []\n",
    "        for syn in wn.synsets(nouns[k], pos=wn.NOUN):\n",
    "            for l in syn.lemmas():\n",
    "                relation.append(l.name())\n",
    "                if l.antonyms():\n",
    "                    relation.append(l.antonyms()[0].name())\n",
    "            for l in syn.hyponyms():\n",
    "                if l.hyponyms():\n",
    "                    relation.append(l.hyponyms()[0].name().split(\".\")[0])\n",
    "            for l in syn.hypernyms():\n",
    "                if l.hypernyms():\n",
    "                    relation.append(l.hypernyms()[0].name().split(\".\")[0])\n",
    "        relation_list[nouns[k]].append(relation)\n",
    "    return relation_list\n",
    "\n",
    "def buildLexicalChain(nouns, relation_list):\n",
    "    lexical = []\n",
    "    threshold = 0.5\n",
    "    for noun in nouns:\n",
    "        flag = 0\n",
    "        for j in range(len(lexical)):\n",
    "            if flag == 0:\n",
    "                for key in list(lexical[j]):\n",
    "                    if key == noun and flag == 0:\n",
    "                        lexical[j][noun] += 1\n",
    "                        flag = 1\n",
    "                    elif key in relation_list[noun][0] and flag == 0:\n",
    "                        syns1 = wn.synsets(key, pos=wn.NOUN)\n",
    "                        syns2 = wn.synsets(noun, pos=wn.NOUN)\n",
    "                        if syns1[0].wup_similarity(syns2[0]) >= threshold:\n",
    "                            lexical[j][noun] = 1\n",
    "                            flag = 1\n",
    "                    elif noun in relation_list[key][0] and flag == 0:\n",
    "                        syns1 = wn.synsets(key, pos=wn.NOUN)\n",
    "                        syns2 = wn.synsets(noun, pos=wn.NOUN)\n",
    "                        if syns1[0].wup_similarity(syns2[0]) >= threshold:\n",
    "                            lexical[j][noun] = 1\n",
    "                            flag = 1\n",
    "        if flag == 0:\n",
    "            dic_nuevo = {}\n",
    "            dic_nuevo[noun] = 1\n",
    "            lexical.append(dic_nuevo)\n",
    "            flag = 1\n",
    "    return lexical\n",
    "\n",
    "def eliminateWords(lexical):\n",
    "    final_chain = []\n",
    "    while lexical:\n",
    "        result = lexical.pop()\n",
    "        if len(result.keys()) == 1:\n",
    "            for value in result.values():\n",
    "                if value != 1:\n",
    "                    final_chain.append(result)\n",
    "        else:\n",
    "            final_chain.append(result)\n",
    "    return final_chain\n",
    "\n",
    "def PreprocessDocuments():\n",
    "    for i in files_path:\n",
    "        f = open(i, \"r\")\n",
    "        dataset = preprocess_text(f.read(), remove_stopwords=True)\n",
    "        # use lexical chains as the feature selection method\n",
    "        nouns = []\n",
    "        l = nltk.pos_tag(dataset)\n",
    "        for word, n in l:\n",
    "            if n == \"NN\" or n == \"NNS\" or n == \"NNP\" or n == \"NNPS\":\n",
    "                nouns.append(word)\n",
    "\n",
    "        relation = buildRelation(nouns)\n",
    "        lexical = buildLexicalChain(nouns, relation)\n",
    "        chain = eliminateWords(lexical)\n",
    "        lexical_chain.append(chain)\n",
    "\n",
    "    global total_features\n",
    "    for features in lexical_chain:\n",
    "        for docfeature in features:\n",
    "            total_features.extend(docfeature.keys())\n",
    "\n",
    "    total_features = list(set(total_features))\n",
    "\n",
    "    for feature in lexical_chain:\n",
    "        temp = []\n",
    "        # print(feature)\n",
    "        for j in total_features:\n",
    "            check = False\n",
    "            for f in feature:\n",
    "                if j in f:\n",
    "                    temp.append(f[j])\n",
    "                    check = True\n",
    "                    break\n",
    "            if not check:\n",
    "                temp.append(0)\n",
    "\n",
    "        final_training_Features.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_50_path = os.getcwd() + \"\\Doc50\"\n",
    "ReadDocuments(doc_50_path)\n",
    "PreprocessDocuments()\n",
    "\n",
    "normalizer = Normalizer()\n",
    "normalize_features = normalizer.fit_transform(final_training_Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Labels {'20361': 0, '20362': 0, '20363': 0, '20364': 0, '20365': 0, '20487': 0, '20488': 0, '20489': 0, '20490': 0, '20491': 0, '52550': 1, '52551': 1, '52552': 1, '52553': 1, '52554': 1, '52555': 1, '52556': 1, '52557': 1, '52558': 1, '52559': 1, '57110': 2, '58043': 2, '58044': 2, '58045': 2, '58046': 2, '58047': 2, '58048': 2, '58049': 2, '58050': 2, '58051': 2, '64830': 3, '64831': 3, '66189': 3, '66322': 3, '66398': 3, '66399': 3, '66400': 3, '66401': 3, '66402': 3, '66403': 3, '101725': 4, '102616': 4, '103117': 4, '103118': 4, '103119': 4, '103120': 4, '103121': 4, '103122': 4, '103123': 4, '103124': 4}\n",
      "Label Sequence [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "Maximum purity of 0.72 found on random state 990\n",
      "K-means lables using lexical chains: [3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 3 0 0 3 0 0 0 0 0 0 0\n",
      " 0 0 0 4 4 4 2 4 4 3 0 3 0]\n",
      "      \n",
      "Purity 0.72\n",
      "      \n",
      "Silhoutte Score: -0.04868462913792616\n"
     ]
    }
   ],
   "source": [
    "SumSqDis = []\n",
    "pca = PCA(n_components=30, random_state=42)\n",
    "pca_vecs = pca.fit_transform(normalize_features)\n",
    "\n",
    "# Purity Score\n",
    "for Path in os.listdir(\"Doc50\" + \"\\\\\"):\n",
    "    file_path = os.getcwd() + f\"\\{'Doc50'}\\\\\" + Path\n",
    "    with open(file_path, \"r\") as file:\n",
    "        FileContents = file.read()\n",
    "        corpus.append(FileContents.lower())\n",
    "        doc_list_sequence.append(Path)\n",
    "\n",
    "actual_labels = ({})  # dictionary to store true assignments for each document | read sequence not followed\n",
    "label_path = os.getcwd() + \"\\Doc50 GT\"\n",
    "for labels_directory in os.listdir(label_path):  # for each assignment folder\n",
    "    actual_cluster = int(\n",
    "        labels_directory[1]\n",
    "    )  # extract cluster label from directory name\n",
    "    doc_labels = os.listdir(\n",
    "        label_path + f\"\\\\{labels_directory}\"\n",
    "    )  # for all document ids assigned to this cluster\n",
    "    for doc in doc_labels:\n",
    "        actual_labels[doc] = actual_cluster - 1  # save cluster label\n",
    "\n",
    "label_seq = []  # save labels in order of documents read\n",
    "for doc in doc_list_sequence:\n",
    "    label_seq.append(actual_labels[doc])\n",
    "\n",
    "\n",
    "print(\"Actual Labels\",actual_labels)\n",
    "print(\"Label Sequence\",label_seq)\n",
    "\n",
    "purity_collection = {}\n",
    "for i in range(1500):\n",
    "    clusters = KMeans(n_init=\"auto\", n_clusters=5, random_state=i, init=\"k-means++\").fit(pca_vecs).labels_\n",
    "    purity_collection[i] = Purity_Score(label_seq, clusters)\n",
    "\n",
    "max_rand_state = max(purity_collection, key=purity_collection.get)\n",
    "print(\n",
    "    f\"Maximum purity of {purity_collection[max_rand_state]} found on random state {max_rand_state}\"\n",
    ")\n",
    "\n",
    "lexicalChainsLabels = KMeans(n_init=\"auto\", n_clusters=5, random_state=max_rand_state, init=\"k-means++\").fit(pca_vecs).labels_\n",
    "\n",
    "print(f\"\"\"K-means lables using lexical chains: {lexicalChainsLabels}\n",
    "      \\nPurity {Purity_Score(label_seq, lexicalChainsLabels)}\n",
    "      \\nSilhoutte Score: {metrics.silhouette_score(final_training_Features, lexicalChainsLabels, metric='euclidean')}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_wordnet(word):\n",
    "    synsets = wn.synsets(word)\n",
    "    return len(synsets) > 0\n",
    "\n",
    "def contains_number(word):\n",
    "    for char in word:\n",
    "        if char.isnumeric():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def min_length_word(word):\n",
    "    if  len(word) in [1,2]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def custom_preprocessor(text):\n",
    "    lematizer = WordNetLemmatizer()\n",
    "    used_terms = {} # keep track of which terms have already been considered\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = []\n",
    "    for word in tokens:\n",
    "        if (not contains_number(word)) and (not min_length_word(word)) and (word not in stopwords.words('english')) and (in_wordnet(word)):\n",
    "            lema_word = lematizer.lemmatize(word)\n",
    "            if lema_word in used_terms.keys():\n",
    "                continue\n",
    "            else:\n",
    "                used_terms[lema_word] = 0\n",
    "                filtered_tokens.append(lema_word)\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "def print_terms(terms):\n",
    "    for term in terms:\n",
    "        print(term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KMeans_Labels(X, n, rstate_limit, true_labels):\n",
    "\n",
    "    # Specify the number of clusters (you can choose an appropriate value)\n",
    "    num_clusters = n\n",
    "    \n",
    "    # find centoids which give maximum purity\n",
    "    purity_collection = {}\n",
    "    for i in range(rstate_limit):\n",
    "        clusters = KMeans(n_init='auto', n_clusters=num_clusters, random_state=i, init='k-means++').fit(X).labels_\n",
    "        purity_collection[i] = Purity_Score(true_labels, clusters)\n",
    "    \n",
    "    max_rand_state = max(purity_collection, key=purity_collection.get)\n",
    "    print(f\"Maximum purity of {purity_collection[max_rand_state]} found on random state {max_rand_state}\")\n",
    "\n",
    "    # Create a KMeans model\n",
    "    kmeans = KMeans(n_init='auto', n_clusters=num_clusters, random_state=max_rand_state, init='k-means++')\n",
    "    # Fit the KMeans model to the TF-IDF data\n",
    "    kmeans.fit(X)\n",
    "    # Get the cluster assignments for each document\n",
    "    cluster_assignments = kmeans.labels_\n",
    "    \n",
    "    return cluster_assignments\n",
    "\n",
    "def Actual_Labels():\n",
    "    actual_labels = {} # dictionary to store true assignments for each document | read sequence not followed\n",
    "    label_path = os.getcwd() + '\\\\Doc50 GT\\\\'\n",
    "    for labels_directory in os.listdir(label_path): # for each assignment folder\n",
    "        actual_cluster = int(labels_directory[1]) # extract cluster label from directory name\n",
    "        doc_labels = os.listdir(label_path + f\"\\\\{labels_directory}\") # for all document ids assigned to this cluster\n",
    "        for doc in doc_labels:\n",
    "            actual_labels[doc] = actual_cluster-1 # save cluster label\n",
    "    \n",
    "    label_seq = [] # save labels in order of documents read\n",
    "    for doc in doc_name:\n",
    "        label_seq.append(actual_labels[doc])\n",
    "    return label_seq\n",
    "\n",
    "def print_results(true_labels, predicted_labels, X):\n",
    "    print(\"RESULTS:\")\n",
    "    print(f\"Purity: {Purity_Score(true_labels, predicted_labels)}\")\n",
    "    print(f\"Silhouette Score: {silhouette_score(X, predicted_labels)}\")\n",
    "\n",
    "def wrapperFunction():\n",
    "    ReadDocuments('Doc50')\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', preprocessor=custom_preprocessor)\n",
    "    X = vectorizer.fit_transform(doc_content)\n",
    "    true_labels = Actual_Labels()\n",
    "    predicted_labels = KMeans_Labels(X, 5, 1500, true_labels)\n",
    "    print_results(true_labels, predicted_labels, X)\n",
    "    return predicted_labels, X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum purity of 0.72 found on random state 1275\n",
      "RESULTS:\n",
      "Purity: 0.72\n",
      "Silhouette Score: 0.0609546220855259\n"
     ]
    }
   ],
   "source": [
    "tfidfLabels, tfidfMatrix = wrapperFunction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consensus Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_consensus_matrix(labels1, labels2):\n",
    "    n = len(labels1)\n",
    "    consensus_matrix = np.zeros((n, n))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            #Calculate the Jaccard similarity between the two label sets\n",
    "            intersection = np.intersect1d(labels1[i], labels2[j])\n",
    "            union = np.union1d(labels1[i], labels2[j])\n",
    "            agreement = len(intersection) / len(union)\n",
    "        \n",
    "\n",
    "            consensus_matrix[i, j] = agreement\n",
    "            consensus_matrix[j, i] = agreement\n",
    "\n",
    "    return consensus_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 50 is out of bounds for axis 0 with size 50",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\FAST\\FYP\\FYP23-Deep-Document-Clustering\\Base Lines\\Lexical Chains\\main_Doc50.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/FAST/FYP/FYP23-Deep-Document-Clustering/Base%20Lines/Lexical%20Chains/main_Doc50.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m consensus_matrix \u001b[39m=\u001b[39m calculate_consensus_matrix(tfidfLabels, lexicalChainsLabels)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/FAST/FYP/FYP23-Deep-Document-Clustering/Base%20Lines/Lexical%20Chains/main_Doc50.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m n_clusters \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m  \u001b[39m# You can adjust this as needed\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/FAST/FYP/FYP23-Deep-Document-Clustering/Base%20Lines/Lexical%20Chains/main_Doc50.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m purity_collection \u001b[39m=\u001b[39m {}\n",
      "\u001b[1;32md:\\FAST\\FYP\\FYP23-Deep-Document-Clustering\\Base Lines\\Lexical Chains\\main_Doc50.ipynb Cell 15\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/FAST/FYP/FYP23-Deep-Document-Clustering/Base%20Lines/Lexical%20Chains/main_Doc50.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/FAST/FYP/FYP23-Deep-Document-Clustering/Base%20Lines/Lexical%20Chains/main_Doc50.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(i, n\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/FAST/FYP/FYP23-Deep-Document-Clustering/Base%20Lines/Lexical%20Chains/main_Doc50.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m#Calculate the Jaccard similarity between the two label sets\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/FAST/FYP/FYP23-Deep-Document-Clustering/Base%20Lines/Lexical%20Chains/main_Doc50.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         intersection \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mintersect1d(labels1[i], labels2[j])\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/FAST/FYP/FYP23-Deep-Document-Clustering/Base%20Lines/Lexical%20Chains/main_Doc50.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         union \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munion1d(labels1[i], labels2[j])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/FAST/FYP/FYP23-Deep-Document-Clustering/Base%20Lines/Lexical%20Chains/main_Doc50.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         agreement \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(intersection) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(union)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 50 is out of bounds for axis 0 with size 50"
     ]
    }
   ],
   "source": [
    "consensus_matrix = calculate_consensus_matrix(tfidfLabels, lexicalChainsLabels)\n",
    "\n",
    "n_clusters = 5  # You can adjust this as needed\n",
    "purity_collection = {}\n",
    "for i in range(1500):\n",
    "    clusters = SpectralClustering(n_clusters=n_clusters, affinity=\"precomputed\", random_state=i).fit(1 - consensus_matrix).labels_\n",
    "    purity_collection[i] = Purity_Score(label_seq, clusters)\n",
    "\n",
    "max_rand_state = max(purity_collection, key=purity_collection.get)\n",
    "print(f\"Maximum purity of {purity_collection[max_rand_state]} found on random state {max_rand_state}\")\n",
    "spectral_labels = SpectralClustering(n_clusters=n_clusters, affinity=\"precomputed\", random_state=max_rand_state).fit(1 - consensus_matrix).labels_\n",
    "print(\"Purity Score: \", Purity_Score(label_seq, spectral_labels))\n",
    "print(\"Sillhouette Coefficient: \",metrics.silhouette_score(pca_vecs, spectral_labels, metric=\"euclidean\"),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum purity of 0.7 found on random state 429\n",
      "Purity:  0.7\n",
      "Sillhouette Coefficient:  0.3519651738586512\n"
     ]
    }
   ],
   "source": [
    "num_topics = 5  # Adjust as needed\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "\n",
    "lda.fit(tfidfMatrix)\n",
    "\n",
    "# Get the topic assignments for each document\n",
    "topic_labels = lda.transform(tfidfMatrix).argmax(axis=1)\n",
    "\n",
    "combined_labels = [lexicalChainsLabels, tfidfLabels, topic_labels]\n",
    "combined_labels = list(map(list, zip(*combined_labels)))\n",
    "\n",
    "normalize_combined_features = Normalizer().fit_transform(combined_labels)\n",
    "topic_purity_collection = {}\n",
    "for i in range(1500):\n",
    "    topic_clusters = (\n",
    "        KMeans(n_init=\"auto\", n_clusters=5, random_state=i, init=\"k-means++\")\n",
    "        .fit(normalize_combined_features)\n",
    "        .labels_\n",
    "    )\n",
    "    topic_purity_collection[i] = Purity_Score(label_seq, topic_clusters)\n",
    "\n",
    "topic_max_rand_state = max(topic_purity_collection, key=topic_purity_collection.get)\n",
    "print(f\"Maximum purity of {topic_purity_collection[topic_max_rand_state]} found on random state {topic_max_rand_state}\")\n",
    "max_labels = (\n",
    "    KMeans(\n",
    "        n_init=\"auto\", n_clusters=5, random_state=topic_max_rand_state, init=\"k-means++\"\n",
    "    )\n",
    "    .fit(normalize_combined_features)\n",
    "    .labels_\n",
    ")\n",
    "print(\"Purity: \", Purity_Score(label_seq, max_labels))\n",
    "print(\"Sillhouette Coefficient: \",metrics.silhouette_score(normalize_combined_features, max_labels, metric=\"euclidean\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
